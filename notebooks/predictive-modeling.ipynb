{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    " * Holdout method (2/3 for training, 1/3 for testing)  \n",
    " * Repeated holdout  \n",
    " * Cross-validation  \n",
    "     * k-fold cross-validation  \n",
    "     * startified k-fold cross-validation (equal representation of classes)  \n",
    "     * leave-one-out  \n",
    " * Bootstrap (sample with replacement)\n",
    " \n",
    "Most commonly: 10 fold cross-validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Steps\n",
    "\n",
    "\n",
    "**Data matrix:** rows are instances/items/patterns and columns are features/attributes.\n",
    "\n",
    "### Statistical Noise  \n",
    "  \n",
    "\n",
    "  Common rule for removing outliers: $ \\text{mean} + 3\\times \\text{std}   $ \n",
    "  \n",
    "  \n",
    "### Dealing with missing values\n",
    "  \n",
    "  * Get rid of them\n",
    "    * Deleting the instances (records)  \n",
    "    * Deleting features with high missing cases  \n",
    "  \n",
    "  * Replacement with mean values  \n",
    "  * Inference / imputations\n",
    "    * \n",
    "    \n",
    "### Feature Transformation\n",
    " \n",
    "  * Combining variables  \n",
    "  * Scaling data  \n",
    "    * Mean centering $x_{new} = x - \\text{mean}(x) $\n",
    "    * Z-score: mean centering and standardization $x_{new} = \\frac{x - \\text{mean}(x)}{\\text{std}(x)}$\n",
    "    * scale to range $[0 .. 1]$ by $x_{new} = \\frac{x - \\min(x)}{\\max(x) - \\min(x)}$\n",
    "    * Log-scaling $x_{new} = \\log(x)$\n",
    "  * Discretization\n",
    "  \n",
    "### Feature selection\n",
    " \n",
    " * Reducing the features with low correletaions ot outcome  \n",
    " * Start from empty set of features and add 1 feature at a time while testing the performance on a sample set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Statistical Modeling\n",
    "\n",
    "Basic assumptions:  \n",
    " * Variables are equally important  \n",
    " * Variables are statistically independent  \n",
    " \n",
    "**Statistical Indepence:** knowledge about the value of one attribute, desn't tell us anything about the value of other attributes.\n",
    "\n",
    "### Naive Bayes Rule\n",
    "\n",
    " * $\\text{Pr}(\\text{Hypothesis}\\ |\\ \\text{Evidence})=\\displaystyle \\frac{\\text{Pr}(\\text{Evidence} \\ | \\ \\text{Hypothesis})\\times \\text{Pr}(\\text{Hypothesis})}{\\text{Pr}(\\text{Evidence})}$\n",
    " \n",
    "Naive Bayes assumption: evidence terms are conditionnaly independet from each other. As a result, evidence can be split into independent parts\n",
    "\n",
    "$$\\text{Pr}(\\text{Evidence} \\ | \\ \\text{Hypothesis}) = \\text{Pr}(\\text{E}_1 \\ | \\ \\text{Hypothesis}) \\times \\text{Pr}(\\text{E}_2 \\ | \\ \\text{Hypothesis}) \\times .. \\times \\text{Pr}(\\text{E}_d \\ | \\ \\text{Hypothesis})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Induction\n",
    "\n",
    " * A method for estimating descrete valued functions  \n",
    " * Robust to noise and missing data  \n",
    " * Applicable to non-linear relationships  \n",
    " \n",
    " #### Basic Frameork\n",
    " \n",
    "   * Each node tests an attribute\n",
    "   * Each branch correpond to some possible attribute values  \n",
    "   * Each leaf node assigns a class  \n",
    "     \n",
    " * Training a decision tree will include the minimum set of attributes in order to fit the data\n",
    " \n",
    " #### Construction of a decision tree\n",
    "   * First attribute is selected as a root node based on given criterion and branches are created\n",
    "   * The instances are split into subsets according to the condition on the node (Divide & Conqure)\n",
    "   * The procedure is repeated recursivly for each branch, by creating new nodes and dividing the data into subsets\n",
    "   * Stop when all the instances at the node have the same class (pure node = leaf node), or we run out of attribues on the path from to the node.\n",
    "   \n",
    " #### Algorithmic Procedure\n",
    "  \n",
    "  * Loop while not done\n",
    "    * At each step, pick the best attribute to split the data\n",
    "    * For each value of the selected attribute, create branches (child nodes)\n",
    "    \n",
    " ### Criterion for selecting the *best* attribute\n",
    " \n",
    " \n",
    "  #### Information gain \n",
    "  \n",
    "  Define entropy: $$\\text{entropy}(p_1, p_2, ..p_n) = -p_1\\log p_1 -p_2 \\log p_2 ... -p_n log p_n$$\n",
    "  \n",
    "  * Total entropy: the weigthed average entropy at each node  \n",
    "  * Information Gain: the entropy before spliting at a node minus total entropy after splitting $$\\text{Gain} = \\text{entropy}_\\text{before splitting} - \\text{entropy}_\\text{after splitting}$$\n",
    "  \n",
    "  #### Gini Index\n",
    "  \n",
    "  ### Overfitting in Decision Trees\n",
    "  \n",
    "  #### Gain Ratio\n",
    "  \n",
    "  Information Gain divided by interinsic information\n",
    "  \n",
    "  * Inrinsic information: entropy of an attribute (not the class)\n",
    "  * The intrinsic informaiton of attribute $v_a$ is higher than $v_b$, if $v_a$ has higher number of possible values \n",
    "  \n",
    "  ### Pruning Decision Trees\n",
    "  \n",
    "  #### Pre-pruning\n",
    "  \n",
    "  #### Post-pruning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "--------\n",
    "\n",
    "  * **Example**\n",
    "  \n",
    "| Index | Age | Marital Status | Highest Degree | Risk Factor  |\n",
    "|-------|-----|----------------|----------------|--------------|\n",
    "| 1     | 22  | Single         | BS             | High         |\n",
    "| 2     | 27  | Married        | MS             | Low          |\n",
    "| 3     | 34  | Single         | MS             | High         |\n",
    "| 4     | 45  | Married        | PhD            | Low          |\n",
    "| 5     | 28  | Married        | PhD            | Low          |\n",
    "| 6     | 32  | Single         | BS             | High         |\n",
    "| 7     | 31  | Married        | PhD            | Low          |\n",
    "| 8     | 38  | Married        | BS             | High         |\n",
    "| 9     | 36  | Single         | MS             | Low          |\n",
    "| 10    | 41  | Married        | MS             | Low          |\n",
    "\n",
    "\n",
    "\n",
    "   * Initial Entropy:\n",
    "   \n",
    "     $\\text{entropy} = -\\frac{4}{10}\\log \\frac{4}{10} - \\frac{6}{10} \\log \\frac{6}{10} = 0.971$\n",
    "\n",
    "   * Split on Marital Status:  \n",
    "   \n",
    "   <img hight=100 width=250 src='figs/decision-tree/DecisionTree-maritalstatus.png'>\n",
    "   \n",
    "      Child 1: Single (3H, 1L) \n",
    "       $\\text{entropy}=-\\frac{3}{4}\\log \\frac{3}{4} - \\frac{1}{4}\\log \\frac{1}{4} = 0.658$  \n",
    "      Child 2: Married (1H, 5L) $\\text{entropy}=-\\frac{1}{6}\\log \\frac{1}{6} - \\frac{5}{6}\\log \\frac{5}{6} = 0.651$\n",
    "      \n",
    "      * Total entropy: $\\text{entropy}_{tot}=\\frac{4\\times 0.658 + 6\\times 0.651}{10} = 0.654$  \n",
    "      \n",
    "      * Information Gain for this node: $\\text{Gain} = 0.971 - 0.654 = 0.317$\n",
    "      \n",
    "      \n",
    "      \n",
    "   * Split on Degree:  \n",
    "   \n",
    "   <img hight=100 width=300 src='figs/decision-tree/DecisionTree-degree.png'>\n",
    "   \n",
    "      Child 1: BS (3H, 0L) $\\text{entropy}=-\\frac{3}{3}\\log \\frac{3}{3} = 0$  \n",
    "      Child 2: MS (1H, 3L) $\\text{entropy}=-\\frac{1}{4}\\log \\frac{1}{4} - \\frac{3}{4}\\log \\frac{3}{4} = 0.811$    \n",
    "      Child 3: PhD (0H, 3L) $\\text{entropy}=-\\frac{3}{3}\\log \\frac{3}{3} = 0$  \n",
    "      \n",
    "      * Total entropy: $\\text{entropy}_{tot} = \\frac{0+4*0.811+0}{10} = 0.324$\n",
    "      * Information Gain for this node: $\\text{Gain} = 0.971 - 0.324 = 0.647$\n",
    "      \n",
    "   * Therefore, splitting on Highest Degree resules in greater information gain.\n",
    "   \n",
    "   In the second step, we add branches to the possible outcome \n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
