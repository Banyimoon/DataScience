{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop Framework for Data Processing\n",
    "====\n",
    "\n",
    "\n",
    "#### Outline\n",
    " * **Data processing in Hadoop**\n",
    "   * Mapper, reducer, and partitioners\n",
    " * **Hadoop I/O**\n",
    "   * InputFormat and OutputFormat\n",
    "   * Readin sequence files\n",
    "   * Compressing the output of mappers and reducers\n",
    " * **Chaining multiple map and reduce tasks**\n",
    " \n",
    "## Hadoop Processing\n",
    "\n",
    "Hadoop splits the input data among the mappers. Each mapper will generate (key,value) pairs, and the intermediate output of mappers is partitioned for the reducers and the partitions are written into disk (local disk to each mapper, not HDFS). \n",
    "\n",
    "#### Partitioner\n",
    "When there are multiple reducers, the output of mappers need to be partitioned. There is a default partitioner that creates partitions by hashing the mappers' output keys:\n",
    "\n",
    "```java\n",
    "job.setPartitionerClass(HashPartitioner.class);\n",
    "\n",
    "public class HashPartitioner<K,V> extends Partitioner<K,V> {\n",
    "    public int getPartition(K ket, V value, int numReduceTasks) {\n",
    "        return (key.hashcode() & Integer.MAX_VALUE) % numReduceTasks;\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example wordcount.java\n",
    "\n",
    "The general scheme of a java program in hadoop is implemented in a main class, which the main class has a mapper class, a reducer class and a main function, as follows\n",
    "\n",
    "<img src=\"hadoop-program.png\" width=200></img>\n",
    "\n",
    "```java\n",
    "public class wordcount {\n",
    "    public class TokenizerMapper extends Mapper<Object, Text, Text, IntWritable> {...}\n",
    "    \n",
    "    public class IntSumReducer extends Reducer<Text, IntWritable, Text, IntWritable> {...}\n",
    "    \n",
    "    public static void main(string[] args) throws Exception {\n",
    "       ...\n",
    "       ...\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InputFormat (for mappers)\n",
    "\n",
    "InputFormat is an abstract class to specify the way input records are defined in input data. By default, the input format is TextInputFormat where each record is a line and the input key is byte offset, and valye is contetn of the line.\n",
    "\n",
    "```java\n",
    "public abstract class InputFormat<K,V> {\n",
    "    public abstract List<inputSplit> getSplit(JobContext context) throws Exception;\n",
    "    \n",
    "    public abstract RecordReader<K,V> {\n",
    "        createRecordReader(inputSplit split,\n",
    "                           TaskAttempt(Context context)) \n",
    "                           throws Exception;\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "| **InputFormat** | **Description** |\n",
    "|:--:|:--|\n",
    "|TextInputFormat|Each line is a record; <br> *key:LongWritable*<br> *value:Text*|\n",
    "|KeyValueTextInputFormat||\n",
    "|SequenceFileInputFormat||\n",
    "|NLineInputFormat||\n",
    "\n",
    "Specifying InputFormat:\n",
    "```java\n",
    "public static void main(String[] args) throws Exception {\n",
    "    Job job = new Job();\n",
    "    ...\n",
    "    job,setInputFormatClass(TextInputFormat.class);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OutputFormat (for reducers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Hadoop Data Types for Keys & Values\n",
    "\n",
    "* **WritableComparable** can be used for both keys & values\n",
    "* **Writable** can be used for values\n",
    "\n",
    "|Class|Description|\n",
    "|:---:|:----:|\n",
    "|BooleanWritable|Standard boolean writable|\n",
    "|ByteWritable|a single byte|\n",
    "|DoubleWritable|a double|\n",
    "|FloatWritable|a float|\n",
    "|IntWritable|an integer|\n",
    "|LongWritable|a long|\n",
    "|Text|to store text using UTF8 format|\n",
    "|NullWritable|Placeholder when the key or value is not needed|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing a Mapper Class\n",
    "\n",
    "* **Context** objects are used to write the output of map() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
