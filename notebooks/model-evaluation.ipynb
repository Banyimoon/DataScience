{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evaluation\n",
    "===="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* **Confusion Matrix**\n",
    "\n",
    "<img src=\"figs/confusion-matrix.png\" width=400></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Precision and Recall**\n",
    "  * **Precision** referes to how many of retrieved items are relevant. I.e., out of items predicted as positive, how many of them are actually positive. \n",
    "  \n",
    "  * **Recall** referes to how many of relevant items are retrieved. The relavant items are the ones that are actually positive. \n",
    "  \n",
    "  $$\\text{Precision} = \\displaystyle \\frac{\\text{TP}}{\\text{Predicted Positive}} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}$$\n",
    "  \n",
    "  $$\\text{Recall} = \\displaystyle \\frac{\\text{TP}}{\\text{Actual Positive}} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}$$\n",
    "  \n",
    "Note: the numerator in both is the same $\\text{TP}$, while the denumerator is different. In precision, the denumerator is the total number of items predicted as positive, and in recall its the number of items that actually belong to the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
