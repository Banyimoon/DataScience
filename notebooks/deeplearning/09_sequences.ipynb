{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modling Sequences in Neural Network\n",
    "======"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " * One-to-one\n",
    " * One-to-many: like generating text description for a single image\n",
    " * Many-to-one: example sentiment analysis\n",
    " * Many-to-many: language translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$W_{hh}$ is shared across time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$h_t = g\\left(W_{hh} h_{t-1} + W_{xh} x\\right)$$\n",
    "\n",
    "$$\\frac{\\partial h_t}{\\partial h_{t-1}} = W_{hh} \\odot g'\\left(W_{hh} h_{t-1} + W_{xh} x\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing gradients\n",
    "\n",
    " * $(\\alpha \\beta)^{t-k}$\n",
    " * if $\\alpha < 1$ the gradients vanish to zero\n",
    " * if $\\alpha>1$, then gradient become too large\n",
    " \n",
    " \n",
    " * Solution:\n",
    "     * Hidden states play like a meory of the system: $h_{t} = h_{t-1} + \\gamma x_t$\n",
    "       It accumulates memory. But, if the time distance is too far, then, we should forget the old stuff\n",
    "       \n",
    "     * $h_{t} = \\theta_t h_{t-1} + \\gamma x_t$:\n",
    "       * To remember something: $\\theta_t = 1$\n",
    "       * To forget something: $\\theta_t = 0$ this means that your memory only depends on current input.\n",
    "       \n",
    "    **Hadamard product** works like a gate:\n",
    "    * inout gate: $i_t$\n",
    "    * activation: $a_t$\n",
    "    * forget gate: $f_t$\n",
    "    * output gate: $o_t$\n",
    "    * $c_t = f_t\\odot c_{t-1} + i_t\\odot a_t$\n",
    "    * $h_t = o_t \\odot Tanh(c_t) = o_t \\odot Tanh(f_t\\odot c_{t-1} + i_t\\odot a_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training deep recurrent neural network, SGD do not work well. Instead, we should Adadelta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0rc4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
