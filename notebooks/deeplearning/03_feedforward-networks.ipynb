{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed-Forward Networks\n",
    "====="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### A basic artifical neuron\n",
    "\n",
    " * **Neuron pre-activation** (or input-activation): $a(\\mathbf{x}) = b + \\sum_i w_i x_i = b + \\mathbf{w}^T\\mathbf{x}$    \n",
    "   $w_i$ are called connection *weights*    \n",
    "   $b$ is *bias*\n",
    " \n",
    " * **Neuran (output) activation:** $h(\\mathbf{x}) = g\\left(a(\\mathbf{x})\\right) = g\\left(b + \\mathbf{w}^T \\mathbf{x}\\right)$     \n",
    "    $g(.)$ is called *activation function*\n",
    "\n",
    "<img src=\"figs/single-neuron.png\" width=\"250\"> </img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Learning in Neural Networks\n",
    "\n",
    "\n",
    " * based on \"Empricial risk minimization\"\n",
    " \n",
    "$$arg \\min_\\theta \\frac{1}{T} l(f(x^t;\\theta), y^t) + \\lambda \\Omega (\\theta)$$\n",
    "\n",
    " * Cost funciton: $l(f(x^t;\\theta), y^t)$ (also called risk)\n",
    " * Regularizer: $\\Omega (\\theta)$\n",
    " * $\\lambda$ is a trade-off between the two terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * NN estimates $f(x)_c = p(y=c\\vert x)$\n",
    "  * we want to maximize $\\displaystyle \\theta^* = arg \\max_\\theta \\frac{1}{T} \\prod_t P(y=c\\vert x_t) + \\lambda \\Omega (\\theta)$\n",
    "  * If we have $\\prod_t P(y=c\\vert x_t)$ it can be re-written as $arg \\max_\\theta \\frac{1}{T} \\sum_t \\log P(y=c\\vert x_t)$. Since $\\log$ is a monotonic increasing funciton.\n",
    "  * another variation: $\\displaystyle \\theta^* =arg \\min \\left(- \\sum_t \\log P(y=c\\vert x_t)\\right)$\n",
    "  \n",
    "  * Numerical instability: since $P(..)$ are small numbers, mulltiplications of such small numbers many times will cause loosing the precision. $\\log$ computation will help with nmerical stability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions for classification\n",
    "\n",
    " * $0-1$ loss: $l()=\\sum_i \\mathcal{I}(\\hat{y}_i = y_i)$\n",
    "   * The most ideal, but not practical for optimization\n",
    "   \n",
    "   \n",
    " \n",
    " * Surrogate loss functions\n",
    "   * Squared loss: $(y-f(x))^2$ (bad loss function, but still used)\n",
    "   * Logistic loss function $\\log \\left(1 + e^{-y f(x)}\\right)$\n",
    "   * Hinge loss: $(1 - yf(x))_+$\n",
    "   * Squared Hinge loss: $(1-yf(x)))^2_+$\n",
    "   \n",
    "Notation: $[x]_+ = \\max (0, x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Loss functions for regression\n",
    "\n",
    " * Euclidean Loss: $\\|y - f(x)\\|_2^2$\n",
    " * Manhattan Loss: $\\|y - f(x)\\|_1$\n",
    " \n",
    "   * Manhattan loss is more aggressive than Euclidean for values close to zero\n",
    "   * But less aggressive for larger values\n",
    " * Huber Loss: $\\left\\{\\begin{array}{lr} 1/2\\|y-f(x)\\|_2^2 & for \\|y-f(x)\\|_2^2 \\| \\delta^2\\\\ \\delta \\|y-f(x)\\|_1 - 1/\\delta^2 & \\text{otherwise}\\end{array} \\right.$\n",
    " \n",
    " * KL divergence (if dealing with probabilities): $\\sum p_i \\log \\frac{p_i}{q_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** In classification, error (risk/missclassification) is determined by product $y_i f(x_i)$. But in regression, it is determined via the difference $y_i-f(x_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Euclidean distance: $\\|x - y\\|_2^2 = (x-y)^T (x-y)$\n",
    " * Mahalanobis metric: $(x-y)^T M (x-y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions for embeddings\n",
    "\n",
    "**Embedding:** We want to map vector $x,y$ into a new space $Z$ to get $z_x,z_y$. Then, we compare $z_x,z_y$ in this new space: $$-1 \\le \\frac{z_x^T z_y}{\\|z_x\\| \\ \\|z_y\\|} \\le +1$$\n",
    "\n",
    "\n",
    " * Cosine distance: $\\frac{x^T y}{\\|x\\|\\ \\|y\\|}$\n",
    " \n",
    " * Triplet loss: $(1 + d(x_i,x_j) - d(x_i,x_k))_+$\n",
    "   * condier triple points: $i,j,k$\n",
    "   * $i,k$ are from the same class, while $i,k$ are from two different classes\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    " * $L_2$ regularization: $\\Omega(\\theta) = \\sum\\sum\\sum W_{i,j}^k$\n",
    "   * gradient: \n",
    "   * \n",
    " * $L_1$ norm: $\\Omega(\\theta) = \\sum\\sum\\sum |W_{i,j}^k|$\n",
    "   * Gradient: \n",
    "   $$sign(W_{i,j}^k) = \\left\\{\\begin{array}{lr} 1 & for\\ W_{i,j}^k>0\\\\-1 &  for\\ W_{i,j}^k<0 \\end{array}\\right.$$\n",
    "   * At zero, we have to use sub-gradien: any line between the \n",
    "   \n",
    " * p-norm:\n",
    "   * $p=\\infty$\n",
    "   * $p=2$ (same as $L_2$ norm)\n",
    "   * $p=1$ (same as $L_1$ norm)\n",
    "   * $0<p<1$\n",
    "   * $p = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Effect of regularizaiton:\n",
    "\n",
    "**Geometric interpretation:**\n",
    "\n",
    " * $\\|y-Wx\\|_2^2$ form some contours. If $W$ is unit vector, in 2-dimensional, contours are circles. Otherwise, in 2-dimensional, contours form elipse, or in higher dimensions, elipsoid.\n",
    " \n",
    " * Regularization: $\\|y - Wx\\|_2^2 + \\lambda \\|W\\|_2^2$\n",
    " \n",
    "   * the solution will be the intersection of above contours, and the contours of unit vectors from the regulariation terms:\n",
    " \n",
    "   * $L_2$: \n",
    "   * $L_1$ always intersect at one of the axes $\\Rightarrow$ some of the parameters will be zero.\n",
    "   * For any $p<2$, they have sharp corners. As a result, we get sparse solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0rc4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
