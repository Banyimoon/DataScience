{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efficient Covolution\n",
    "======"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receptive Field\n",
    "\n",
    "\n",
    "Consider image classification: cat vs. dog?\n",
    "\n",
    " * Ideally, we want to look at all the pixels. But this results in too many wegiths.\n",
    " * Practically, we break the image into patches, and use local connectivity concept to reduce the number of weights.\n",
    " \n",
    "\n",
    "The window \n",
    "\n",
    " Intuition: one element i the patch corresponds to how many original elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: $x_{n\\times n}\\rightarrow []_{m\\times m} \\rightarrow ReLU_{m\\times m} \\rightarrow \\text{pooling layer}(2\\times 2) $\n",
    "\n",
    " * Convolution\n",
    " * Non-linear activation: ReLU\n",
    " * Pooling\n",
    "\n",
    "\n",
    "Then, one value in the final output above corresponds to $2m\\times 2m$ elments in $x$. So, the receptive field is $2m\\times 2m$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **AlexNet: **\n",
    "\n",
    "The very first paper that showed deep convolutional network works well in image classification: *\"Imagenet Classification with Deep Convolutional Neural Networks\", NIPS 2012*\n",
    "\n",
    " * 5 convolutional layers, 3 fully connected\n",
    " * 60 million parameters.\n",
    " * GPU-based training (massive amount of parallelism)\n",
    "   * 60 million parameters won't fir in GPU mempry $\\Rightarrow$ use 2 GPUs\n",
    "   * split the model into half, one part of the model is on 1 GPU, the other on second GPU\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **VGG**\n",
    "\n",
    "*Very Deep Convolutional Networks dor Large Scale image Recognition\", Arxiv 1409,1556*\n",
    "\n",
    " * 19 layers deep, 3 fully connected layers\n",
    " * 144 million parameters\n",
    " * $3\\times 3$ convolutional filters with sride $1$: (they showed that smaller filters works better)\n",
    " * $2\\times 2$ max-pooling layers with stride $2$\n",
    " * Established that smaller filters (are parameter efficient) and deeper networks are better\n",
    " \n",
    "\n",
    "\n",
    "  * If we have two filters of size $n$ and $m$: the receptive filed will be $(n+m-1)\\times (n+m-1)$\n",
    "  \n",
    "  Consider two cases below:\n",
    "    * two filters of size $3\\times3$ and $3\\times 3$ has $5\\times5$ receptive filed: $[filter]_{3\\times 3} \\longrightarrow [ReLU] \\longrightarrow [filter]_{3\\times3}$\n",
    "    * Using a single filter of size $5\\times5$ also has the same receptive\n",
    "    \n",
    "    * But the first option has more non-linearity, between them\n",
    "     * the more non-linearity we have, the more \n",
    "     \n",
    "**Take home message:** Do not use filters more than size $3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **GoogleLeNet**\n",
    "\n",
    "*Going Deeper with Convolutions*, CVPR 2015\n",
    "\n",
    " * Multiple networks together\n",
    " * 22 layers, introduced the \"inception\" module\n",
    " * Efficient architecture\n",
    " \n",
    " \n",
    "#### Inception module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* $1\\times 1$ convolution is a saclar multiplication: $[w]_{1\\times1}=\\alpha$\n",
    "  $$y[i] = \\sum_k x[i-k] w[k] = \\alpha x[i]\\ \\ \\ \\ \\ \\Longrightarrow \\text{scalar multiplcation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Input image: $C\\times H\\times W$\n",
    " \n",
    " $N\\ \\ 1\\times1 convolution$ : $N\\times C\\times 1\\times1$\n",
    " \n",
    " Output: $N\\times H\\times W$ (this is because n+m-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0rc4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
