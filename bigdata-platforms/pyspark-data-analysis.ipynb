{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark for Data Analysis\n",
    "============\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Basic Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[1] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext \n",
    "sc = SparkContext( 'local', 'pyspark')\n",
    "\n",
    "int_RDD = sc.parallelize(range(10), 3)\n",
    "\n",
    "int_RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_RDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2], [3, 4, 5], [6, 7, 8, 9]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_RDD.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data from a text file\n",
    "\n",
    "To read data from a local file, you need to specify the address by **file://**\n",
    "\n",
    "```\n",
    "   textFile(\"file:///home/vahid/examplefile.txt\")\n",
    "```\n",
    "\n",
    "But if the file is on HDFS, then we can specify the address by\n",
    "```\n",
    "    textFile(\"/user/wordcount/input/examplefile.txt\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[8] at textFile at NativeMethodAccessorImpl.java:-2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = sc.textFile('file://data/31987-0.txt')\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Narrow Transformation\n",
    "\n",
    "\n",
    "  * **map:** applies a function to each element of RDD.\n",
    "\n",
    "  * **flatMap:** similar to map, except that here we can have 0 or more outputs for each element\n",
    "  \n",
    "  * **filter:** apply a boolean function to each element of RDD, resulting in filtering out based on that function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example = sc.textFile(\"file://data/example.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wide Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation and Configuration for Ubuntu\n",
    "\n",
    "\n",
    "Dowload and extract the spark package from \n",
    "\n",
    "\n",
    "```\n",
    "tar xvfz spark-1.5.2-bin-hadoop2.6.tgz\n",
    "sudo mv spark-1.5.2-bin-hadoop2.6 $HOME/apps/spark/\n",
    "cd $HOME/apps/spark/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to add the SPARK_HOME location to the PATH environment variable\n",
    "\n",
    "```\n",
    "export SPARK_HOME=$HOME/apps/spark \n",
    "export PATH=$SPARK_HOME/bin:$PATH \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can launch pyspark by ```pyspark```\n",
    "\n",
    "<img src='pyspark-launch.png' >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce the verbosity level\n",
    "\n",
    "By default, pyspark will generate lots of log messages when you run some command, and we can see how that can be a problem. To reduce the verbosity, copy the template file in the conf folder \n",
    "\n",
    "```cp $SPARK_HOME/conf/log4j.properties.template $SPARK_HOME/conf/log4j.properties```\n",
    "\n",
    "and edit it by replacing the INFO to WARN.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Using pyspark in iPython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In order to use pyspark in an iPython notebook, you need to configure it by adding a new file in the startup directory of ipython profile.\n",
    "\n",
    "```\n",
    "vim $HOME/.ipython/profile_default/startup/00-pyspark-setup.py\n",
    "```\n",
    "and add these contents in this file:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Configure the environment\n",
    "if 'SPARK_HOME' not in os.environ:\n",
    "    home_folder = os.environ['HOME']\n",
    "    os.environ['SPARK_HOME'] = os.path.join(home_folder, 'apps/spark')\n",
    "\n",
    "# Create a variable for our root path\n",
    "SPARK_HOME = os.environ['SPARK_HOME']\n",
    "\n",
    "# Add the PySpark/py4j to the Python Path\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, \"python\", \"build\"))\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, \"python\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, you should be able to run ipython and use pyspark. Try running the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vahid/apps/spark\n"
     ]
    }
   ],
   "source": [
    "print(SPARK_HOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[1] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext( 'local', 'pyspark')\n",
    "\n",
    "sc.parallelize(range(10), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you received an error for py4j, then you also need to run these two commands in bash shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```\n",
    "export PYTHONPATH=$SPARK_HOME/python/:$PYTHONPATH\n",
    "export PYTHONPATH=$SPARK_HOME/python/lib/py4j-0.8.2.1-src.zip:$PYTHONPATH\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
