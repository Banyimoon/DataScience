{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative Models: Restricted Boltzmann Machines\n",
    "===="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The main difference:**\n",
    "\n",
    " * Previous models: directed graph $x \\to z \\to \\tilde{x}$   \n",
    " \n",
    " * RBM: $x -- z$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Energy Function\n",
    "\n",
    " * Notation: here we use $h$ for latent variable\n",
    "\n",
    "$$E(x,h) = -h^T Wx - c^Tx - b^Th \\\\ = -\\sum \\sum W_{i,j}h_jx_k - \\sum c_k x_k- \\sum b_j h_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution:\n",
    "\n",
    " * Joint distribution\n",
    " \n",
    " $$p(x,h) = \\frac{\\exp(-E(x,h))}{Z}$$\n",
    " \n",
    " * Partition function: $Z = \\sum_{x\\in \\{0,1\\}^n} \\sum_{h\\in\\{0,1\\}^m} p(x,h)$\n",
    " \n",
    " * Intractable: because we need to sum of $2^n$ $x$s and $2^m$ $h$s, therefore total of $2^{n+m}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphical model\n",
    "\n",
    " $$p(x,h) = \\frac{\\exp(-E(x,h))}{Z} = \\frac{\\exp(h^TWx + c^Tx + b^th)}{Z} \\\\ = \\frac{\\exp() \\exp() \\exp()}{Z}$$\n",
    " \n",
    "**Connection to physics and nature:**\n",
    " * Interactions between atoms and molecules\n",
    " * Differnet energy functions\n",
    " \n",
    " $$p(x,h) = \\frac{1}{Z} \\prod \\prod \\exp() \\times \\prod \\exp() \\times \\prod \\exp()$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    " * Conditional dist\n",
    " \n",
    " $$p(h|x) = \\prod_j p(h_j|x)$$\n",
    " \n",
    " $p(h_j=1|x) = \\frac{1}{1 + \\exp(-(b_j + W_{j.}x))} = sigmpoid(b_j + W_{j.}x)$\n",
    " \n",
    " \n",
    " $$p(x|h) = \\prod p(x_k|h)$$\n",
    " \n",
    " $p(x_k=1|h) = \\frac{1}{1 + \\exp()} = sigmoid(x_k + h^T W_{.k})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Derivation:**\n",
    "\n",
    "$$p(h|x) = \\frac{}{\\sum_\\tilde{h} p(x,\\tilde{h})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Marginal distribution\n",
    " \n",
    "$$p(x) = \\sum_h p(x,h) = \\sum_h \\frac{\\exp(-E(x,h))}{Z}$$\n",
    "\n",
    "$$p(x) = \\frac{\\exp(-F(x))/}{Z}$$\n",
    "\n",
    "$F(x)$ is the **free energy**.\n",
    "\n",
    "This is called **softplus(.)**. Softplus is a smooth version of ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    " * Loss function: negative log-likelihood   \n",
    " \n",
    " $$\\frac{1}{T} \\sum_{t\\in \\text{training}} l(f(x^t)) = \\frac{1}{T} \\sum_t -\\log(p(x^t))$$\n",
    " \n",
    " * Training: stochastic gradient descent\n",
    " \n",
    " where $\\mathbf{E}_x$ is the expectaiton under distrubution of $x$ and $E$ is the energy function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall: computing expectation:\n",
    "\n",
    " * Compute expectation $\\mathbf{E}_p[f(X)]$?\n",
    " \n",
    " $$\\mathbf{E}[f(x)] = \\int_x p(x) f(x) dx$$\n",
    " \n",
    " We sample from the distribution of $X$ : ($(p(x)) and then we take the average: \n",
    " \n",
    " $$\\mathbf{E}[f(x)] = \\int_x p(x) f(x) dx = \\frac{1}{|S|} \\sum_s\\in p(x)f(s)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contrastive Divergence\n",
    "\n",
    "\n",
    " \n",
    " Sampling the negative samples.\n",
    " \n",
    "  * Replace expectation by a point estimate $$\n",
    "\n",
    "  * Obtain the point $$ by Gibbs sampling\n",
    "  \n",
    "  * Start sampling chain at $x^t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding more layers: **Deep Boltzmann Machines**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion: comparing modeling using directed graph vs. undirected\n",
    "\n",
    "\n",
    " * Directed: $z \\to x$ we model $p(x|z)$   \n",
    " \n",
    " * Undirected: $z -- x$ we model $p(x,z)$ and $p(z)$\n",
    " \n",
    " The directed version is easier, since we use some input.\n",
    " \n",
    " The undirected graph should in theory give more accurate model since during the iterative process, we repeatedly make both $x$ and $z$ better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Bernoulli RBM\n",
    "\n",
    " * For the case when input $x$ is real and unbounded:\n",
    "   * add a quadratic termto the energy function\n",
    " $$E(x,h) = -h^TWx - c^Tx - b^Th +/frac{1}{2} x^Tx$$\n",
    "   \n",
    "   * $p(x|h)$ is now a Guassian dist\n",
    "     * $\\mu = c + W^Tx$  \n",
    "     * $\\Sigma = I$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0rc4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
