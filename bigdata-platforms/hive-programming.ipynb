{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hive Programming \n",
    "=====\n",
    "\n",
    "Outline:\n",
    " * \n",
    " * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Metastore** a data base for metadata\n",
    "* **HiveQL + compiler** a query language for interacting with Hive\n",
    "* **Hive Shell** an interactive shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure\n",
    "\n",
    "* **Managed Table**\n",
    "* **External Table** when creating an external table, use keyword *external* and specify the location to store the table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create table \n",
    "\n",
    "\n",
    "#### Create table from an exisiting table\n",
    "\n",
    "```sql\n",
    "CREATE \n",
    "```\n",
    "\n",
    "#### Paritioning tables using PARTITIONED BY\n",
    "\n",
    " * ** Creating partitioned tables**\n",
    " \n",
    "```sql\n",
    "CREATE TABLE table_name (\n",
    "\n",
    ")\n",
    "PARTITIONED BY (attribute_id type)\n",
    "```\n",
    " * ** Loading partitioned tables:**\n",
    "\n",
    "```\n",
    "load data local inpath \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert\n",
    "\n",
    "#### Multi-table inserts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval queries\n",
    "\n",
    "```sql\n",
    "SELECT [ALL|DISTINCT] select_expr, ...\n",
    "FROM table_name,\n",
    "[WHERE condition_statement, ...]\n",
    "[GROUP BY ]\n",
    "[ORDER BY ]\n",
    "[CLUSTER ]\n",
    "   | [DISTRIBUTE BY ] [SORT BY column_list]\n",
    "[LIMIT ];\n",
    "```\n",
    "\n",
    " * **SORT BY**\n",
    "\n",
    " * **DISTRIBUTE BY**\n",
    " for map-reduce scripts\n",
    " \n",
    " * **CLUSTER BY**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join queries\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Example: \n",
    "\n",
    "**Create a table**\n",
    "\n",
    "```sql\n",
    "hive> create table wiki_edit (\n",
    "    >     revision STRING,\n",
    "    >     ArticleID BIGINT,\n",
    "    >     RevisionID BIGINT,\n",
    "    >     Article STRING,\n",
    "    >     EditTime STRING,\n",
    "    >     UserName STRING,\n",
    "    >     UserID BIGINT)\n",
    "    > row format delimited\n",
    "    > fields terminated by ' '\n",
    "    > stored as textfile;\n",
    "```\n",
    "\n",
    "** Retirieval**\n",
    "\n",
    "```\n",
    "Select DISTINCT UserName\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hive command line \n",
    "\n",
    "```\n",
    "linux> hive -e \n",
    "```\n",
    "silent mode:\n",
    "```\n",
    "linux -S -e \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hive versus Pig\n",
    "\n",
    "Let's review hive and pig:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Mappers and Reducers for Hive\n",
    "\n",
    "Let's say we have a table on hive, and we want to process this table.\n",
    "\n",
    "exmaple: word-count in hive, the challenge is we cannot split the words directly in hive. Therefore, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original table\n",
    "\n",
    "```\n",
    "FROM (source_table\n",
    "\n",
    ") mapper_output\n",
    "Insert OVERWRITE TABLE output_table\n",
    "(!apply reduce!);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "\n",
    "```\n",
    "<139 hadoop2:~/web/cse491/14 >hive -e 'select count(distinct category)-1 from bestbuy'\n",
    "WARNING: org.apache.hadoop.metrics.jvm.EventCounter is deprecated. Please use org.apache.hadoop.log.metrics.EventCounter in all the log4j.properties files.\n",
    "Logging initialized using configuration in jar:file:/usr/local/hive-0.10.0/lib/hive-common-0.10.0.jar!/hive-log4j.properties\n",
    "Hive history file=/tmp/mirjalil/hive_job_log_mirjalil_201603141909_421404800.txt\n",
    "Total MapReduce jobs = 1\n",
    "Launching Job 1 out of 1\n",
    "Number of reduce tasks determined at compile time: 1\n",
    "In order to change the average load for a reducer (in bytes):\n",
    "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
    "In order to limit the maximum number of reducers:\n",
    "  set hive.exec.reducers.max=<number>\n",
    "In order to set a constant number of reducers:\n",
    "  set mapred.reduce.tasks=<number>\n",
    "Starting Job = job_201602280853_1385, Tracking URL = http://hadoophead2:50030/jobdetails.jsp?jobid=job_201602280853_1385\n",
    "Kill Command = /usr/local/hadoop-1.1.1/libexec/../bin/hadoop job  -kill job_201602280853_1385\n",
    "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\n",
    "2016-03-14 19:09:15,209 Stage-1 map = 0%,  reduce = 0%\n",
    "2016-03-14 19:09:19,261 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.14 sec\n",
    "2016-03-14 19:09:20,274 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.14 sec\n",
    "2016-03-14 19:09:21,286 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.14 sec\n",
    "2016-03-14 19:09:22,294 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.14 sec\n",
    "2016-03-14 19:09:23,302 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.14 sec\n",
    "2016-03-14 19:09:24,311 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.14 sec\n",
    "2016-03-14 19:09:25,320 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.14 sec\n",
    "2016-03-14 19:09:26,329 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.14 sec\n",
    "2016-03-14 19:09:27,337 Stage-1 map = 100%,  reduce = 33%, Cumulative CPU 3.64 sec\n",
    "2016-03-14 19:09:28,347 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5.19 sec\n",
    "2016-03-14 19:09:29,356 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5.19 sec\n",
    "2016-03-14 19:09:30,365 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5.19 sec\n",
    "MapReduce Total cumulative CPU time: 5 seconds 190 msec\n",
    "Ended Job = job_201602280853_1385\n",
    "MapReduce Jobs Launched: \n",
    "Job 0: Map: 1  Reduce: 1   Cumulative CPU: 5.19 sec   HDFS Read: 12162089 HDFS Write: 5 SUCCESS\n",
    "Total MapReduce CPU Time Spent: 5 seconds 190 msec\n",
    "OK\n",
    "1231\n",
    "Time taken: 26.135 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
