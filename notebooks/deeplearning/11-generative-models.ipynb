{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative Models\n",
    "===\n",
    "\n",
    "  * Unsupervised learning: only use input data\n",
    "  \n",
    "##### Example: the most basic generative model: PCA\n",
    "\n",
    " $$X = \\mu + \\phi \\alpha$$\n",
    " \n",
    " where $\\mu$ is the mean, and $\\alpha$ is the eigen vectors. By generating different coeffcitns $\\alpha$, we can generate new $X$ values.\n",
    " \n",
    "#### Differnet techniques for unsupervised learning\n",
    " * Autoencoders\n",
    " * Generative Adversarial Networks\n",
    " * Restricted Boltzman Machines\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Generative Models?\n",
    "\n",
    " * Beyond associating inputs to putputs\n",
    " \n",
    " * Recognize objects in the world and their factors of variation: recgnize a car and its differnt forms doors open or closed\n",
    " \n",
    " * Understand and imagine how the world evolves\n",
    " \n",
    " * Detect surprising events in the world\n",
    " \n",
    " * Imagine and generate rich plans for future\n",
    " \n",
    " * Establish concepts as useful for reasoning and decision making"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What we will learn?\n",
    "\n",
    " * $f_\\theta()$ use deep networks\n",
    "   * fully connected\n",
    "   * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto-encoders\n",
    "\n",
    " * Decoder\n",
    " \n",
    " * Encoder\n",
    " \n",
    " We can share the weothd so that $W^* = W^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * If the number of hiddent units is the same as input and output units, then using an identity matrix as weights can re-generate the same input as output.\n",
    " \n",
    " * To avoid this, we can make the number o fhidden units smaller, so then the network should learn useful informaiton from input.\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    " * For binary inputs: cross-0entropy loss\n",
    " \n",
    " * For real-valued inputs: sum-of-squared differences\n",
    "   * Use a linear activation function at the output \n",
    " \n",
    "### Gradients:\n",
    "\n",
    " * in both cases above: \n",
    " \n",
    " * When weights are shared (tied/coupled), use the sum of .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undercomplete Hidden Layers\n",
    "\n",
    " * When hidden layer is smaller than the input layer\n",
    "   * Hidden layer compresses the input\n",
    "   * Will compress well ony for the training distribution\n",
    "   \n",
    "      * Example: Good compressed features for input digit if it is trained for MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overcomplete Hidden Layer\n",
    "\n",
    " * Will learn identity matrux\n",
    "  * No compression\n",
    "  \n",
    " * This will only be useful in the following scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Autoencoder\n",
    "\n",
    " * If the decoder is linear, what is the best encoder for MSE loss?\n",
    " \n",
    "  **Theorem:** let $A$ be any matrix, with SVD decompositon: $A = U\\Sigma V^T$\n",
    "  \n",
    "    * decomposition of rank $K$: \n",
    "    \n",
    "    $h(X) = V^T_{\\le K}$\n",
    "    \n",
    "    $h(X) = f(\\tilde{W} X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimality\n",
    "\n",
    " * If the inputs are normalized (by subtracting the mean):\n",
    "   * then the encoder corresponds to PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### A Probabilistiv Viewpoint\n",
    "\n",
    "**Goal:** modeling $p(x)$\n",
    "\n",
    "Three ways to do this:\n",
    "\n",
    " * Fully observed models\n",
    "   * An undirected graphical model\n",
    "   * here there ar eno latent variable, so we can directly model the joint distribution\n",
    "   * Example: recurrent neural network\n",
    "  \n",
    " * Transformation models: \n",
    "   * start by a random vector $z$ that can be generated $z\\sim \\mathcal{N}(0,I)$, then we learn a transformation funciton $f$ so that maps $z \\to x = f(z)$\n",
    "   * transform un-observed noise source using a parametrized funtion\n",
    "   * Example: many sampling functions, Generative Adversarial Networks (GAN)\n",
    "  \n",
    " \n",
    " * Latent Variable Model: both $x,z$ are random varibale\n",
    "   * modeling hidden causes\n",
    "   * introduc unobserved local random variable\n",
    "   * \n",
    " \n",
    "**Review on graphical model:** to get the joint probability distribution $p(x_1, ...,x_n)\n",
    " * Directed graphical model\n",
    " * Undirected graphical model (here we have to deal with the partition function $Z$)\n",
    " \n",
    " Directed grpahical model captiures the independendce assumption:\n",
    " \n",
    " Example:\n",
    "  $x_1\\to x_2 \\ \\ x_2\\to x_3 \\ \\ x_2 \\to x_4 \\ \\ x_4\\to x_3$\n",
    "  \n",
    "  $p(x_1,x_2,x_3,x_4) = p(x_1)~p(x_2|x_1)~p(x_3|x_2x_4)~p(x_4|x_2)$\n",
    "  \n",
    "  iIn general, we can write: $p(x_1,...x_n) = \\prod_{i=1}^n p(x_i|\\pi(x_i))$ where $\\pi$ rep[resents the parents of a node.\n",
    "  \n",
    "  * For undirected:\n",
    " \n",
    " $$p(x_1,...x_n) = \\frac{\\prod_{i=1}^m f_i (\\phi(x))}{Z}$$\n",
    " \n",
    "in undirected graphical model, we have to deal with the partition function $Z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferential Problems\n",
    "\n",
    " 1. ** Evicence Estimation**\n",
    "\n",
    " $$p(x) = \\int p(x,z) dz$$\n",
    "\n",
    " 2. ** Moment computation:**\n",
    " \n",
    " $$\\mathbf{E}[f(x)|z] = \\int f(z) p(z|x) dz$$\n",
    " \n",
    " 3. **Prediction**\n",
    " \n",
    " $$\\mathbf{E}[x_{t+1}] = p(x_{t+1}|x_t) p(x_t) dx_t$$\n",
    " \n",
    " 4. **Hypothesis testing:**\n",
    " \n",
    " $$\\mathcal{B} = \\log p(x|H_1) - \\log p(x|H_2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance sampling\n",
    "\n",
    "$$w^{(s)} = $$ \n",
    "\n",
    "$w^{(s)}$ is called the weight/iportance. Intuitivly speaking, it tells how the estimated function $q(z)$ matches with the real distrin=bution $p(z)$\n",
    "\n",
    "Then, with sampling, we convert the integral into a summation:\n",
    "\n",
    "$$p(x) = \\frac{1}{S}\\sum_{s}w^{(s)} p(x|z^{(s)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance Sampling to Variational Inference\n",
    "\n",
    "For this, we use Jensen's inequality: $$\\log\\left(\\int p(x) q(x) dx\\right) \\ge \\int p(x) \\log q(x) dx$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Generative Models\n",
    "\n",
    "\n",
    " * Goal: we want to find the joint distribution: $$P(x,z)$$\n",
    " \n",
    " For this, we need to know\n",
    " \n",
    "   * $P(x)$\n",
    "   * $P(x|z)$\n",
    "   * $P(z|x)$\n",
    "   * $P(z)$\n",
    "   \n",
    "  * Easiest one to solve first is: $P(x)$\n",
    " \n",
    "  * Hardest one is $P(z|x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Adversarial Networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Auto-Encoders\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * We start by input data/image ($x$)\n",
    " * Using the inference network, we model parmaters of the latern variable $z$: for example $q(z|x) \\sim \\mathcal{N}(\\mu_z,\\Sigma_z)$\n",
    " * Using the parmaters obtained above, generate a random $z$\n",
    " * Use the generated $z$ to reconstruct and obtain $\\hat{x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0rc4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
