{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Neural Nework (CNN)\n",
    "===\n",
    "\n",
    "Consider image recognition problem. We want a neural network with the following requirements:\n",
    " * must deal with very high dimensional data\n",
    " * can explit the 2D topology of pixels. Neighboring pixels are not completely random, rather, they are smomehow related.\n",
    " * can build invariance to certain variations \n",
    "   * translations\n",
    "   * illuminations\n",
    "   * noise\n",
    "   \n",
    "Convolutionl neural networks leverage these ideas\n",
    " * local connectivity\n",
    " * parameter sharing\n",
    " * pooling / subsampling hidden units\n",
    " \n",
    "### Local connectivity:\n",
    "\n",
    " * Use local connectivity of hidden units (instead of all units)\n",
    " \n",
    "  * Each hidden unit is connected to only a subregion (patch) of image\n",
    "  * connected to all channels: 1 for grayscae, RGB ..\n",
    "  \n",
    " * Reduces the number of weights\n",
    " \n",
    "### Parameter sharing\n",
    "\n",
    " * Reduces the number of parameters\n",
    " * extracts the same features at every position (features are \"equivalet\")\n",
    " \n",
    "Feature map:  $\\displaystyle \\mathbf{y}_j = \\sigma\\left(\\sum_k \\mathbf{w}_{ik} * \\mathbf{x}_i \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution Operation\n",
    "\n",
    " * Continous convolution\n",
    "\n",
    "$$s(t) = (f * w)(t) = \\int f(x) w(t-x) dx$$\n",
    "\n",
    " * Discrete convolution\n",
    "\n",
    "$$s(t) = (f*w)(t) = \\sum_{a=-\\infty}^{+\\infty} f(a) w(t-a)$$\n",
    "\n",
    "\n",
    "   * The convolution of $\\mathbf{x}$ with weights $\\mathbf{w}$ is computed as foloows\n",
    "\n",
    "       $\\displaystyle y[i] = \\sum_k x[i-k] w[k]$   \n",
    "       flip $\\mathbf{x}$, and slide it over $\\mathbf{w}$.   \n",
    "\n",
    "       $\\displaystyle y[i] = \\sum_k x[k] w[i-k]$\n",
    "\n",
    "If we have a $2\\times2$ and $2\\times 2$, then we get a $3\\times 3$.\n",
    "\n",
    "   * (related concept): the correlation of $\\mathbf{x}$ with weights $\\mathbf{w}$ is\n",
    "   \n",
    "       $\\displaystyle y[i] = \\sum_k x[i+k] w[k]$   \n",
    "       $\\displaystyle y[i] = \\sum_k x[i] w[i+k]$   \n",
    "       with correlation, we do not flip any vector\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flipping kernel vs not flipping\n",
    "\n",
    " * Cumulative property\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________\n",
    "\n",
    "### Example:\n",
    "\n",
    " * Convolution:\n",
    " \n",
    "$$\\displaystyle \\left[\\begin{array}{ccc}0&20&40\\\\80&20&0\\\\0&0&40 \\end{array}\\right] * \\left[\\begin{array}{cc}0&0.5 \\\\ 0.25 & 1 \\end{array}\\right] = ?$$\n",
    "\n",
    "Flip $\\left[\\begin{array}{cc}0&0.5 \\\\ 0.25 & 1 \\end{array}\\right]$ on both axes: $\\left[\\begin{array}{cc}1 &0.25 \\\\ 0.5 & 0 \\end{array}\\right]$\n",
    "\n",
    "Then, we can use simple element-wise multiplications followed by summing over the products:\n",
    "\n",
    "First row:\n",
    "\n",
    "$$\\left[\\begin{array}{cc}0&0\\\\0 & 0 \\end{array}\\right]  \\left[\\begin{array}{cc}1 &0.25 \\\\ 0.5 & 0 \\end{array}\\right] = $$\n",
    "\n",
    "$$\\left[\\begin{array}{cc}0&0\\\\0 & 20 \\end{array}\\right]  \\left[\\begin{array}{cc}1 &0.25 \\\\ 0.5 & 0 \\end{array}\\right] = $$\n",
    "\n",
    "$$\\left[\\begin{array}{cc}0&0\\\\20 & 40 \\end{array}\\right]  \\left[\\begin{array}{cc}1 &0.25 \\\\ 0.5 & 0 \\end{array}\\right] = $$\n",
    "\n",
    "$$\\left[\\begin{array}{cc}0&0\\\\40 & 0 \\end{array}\\right]  \\left[\\begin{array}{cc}1 &0.25 \\\\ 0.5 & 0 \\end{array}\\right] = $$\n",
    "\n",
    " second row:\n",
    "$$\\left[\\begin{array}{cc}0&0\\\\0 & 80 \\end{array}\\right]  \\left[\\begin{array}{cc}1 &0.25 \\\\ 0.5 & 0 \\end{array}\\right] = $$\n",
    "\n",
    "$$\\left[\\begin{array}{cc}0&20\\\\80 & 20 \\end{array}\\right]  \\left[\\begin{array}{cc}1 &0.25 \\\\ 0.5 & 0 \\end{array}\\right] = $$\n",
    "\n",
    "$$\\left[\\begin{array}{cc}20&40\\\\20 & 0 \\end{array}\\right]  \\left[\\begin{array}{cc}1 &0.25 \\\\ 0.5 & 0 \\end{array}\\right] = $$\n",
    "\n",
    "$$\\left[\\begin{array}{cc}40&0\\\\0 & 0 \\end{array}\\right]  \\left[\\begin{array}{cc}1 &0.25 \\\\ 0.5 & 0 \\end{array}\\right] = $$\n",
    "\n",
    "$$\\left[\\begin{array}{cc}0&0\\\\0 & 0 \\end{array}\\right]  \\left[\\begin{array}{cc}1 &0.25 \\\\ 0.5 & 0 \\end{array}\\right] = $$\n",
    "\n",
    "Third row:\n",
    "$$\\left[\\begin{array}{cc}0&0\\\\0 & 0 \\end{array}\\right]  \\left[\\begin{array}{cc}1 &0.25 \\\\ 0.5 & 0 \\end{array}\\right] = $$\n",
    "\n",
    "$$\\left[\\begin{array}{cc}0&0\\\\0 & 0 \\end{array}\\right]  \\left[\\begin{array}{cc}1 &0.25 \\\\ 0.5 & 0 \\end{array}\\right] = $$\n",
    "\n",
    "$$\\left[\\begin{array}{cc}0&40\\\\0 & 0 \\end{array}\\right]  \\left[\\begin{array}{cc}1 &0.25 \\\\ 0.5 & 0 \\end{array}\\right] = $$\n",
    "\n",
    "$$\\left[\\begin{array}{cc}40&0\\\\0 & 0 \\end{array}\\right]  \\left[\\begin{array}{cc}1 &0.25 \\\\ 0.5 & 0 \\end{array}\\right] = $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Correlation:\n",
    "\n",
    "$$\\displaystyle \\left[\\begin{array}{ccc}0&20&40\\\\80&20&0\\\\0&0&40 \\end{array}\\right] \\star \\left[\\begin{array}{cc}0&0.5 \\\\ 0.25 & 1 \\end{array}\\right] = ?$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling\n",
    "\n",
    " * Pool responcses of hidden units in the same neighborhppd\n",
    " * Pooling is performed in non-overlapping neighborhoods (sab-sampling)\n",
    " \n",
    "Advantages:\n",
    "\n",
    " * invariance to local translations\n",
    " * \n",
    " \n",
    "#### Max-pooling\n",
    "\n",
    "\n",
    "### Example:\n",
    "\n",
    "$$\\displaystyle \\left[\\begin{array}{cccc}0.3&0.2&1&0.1\\\\3&0.8&0.8&2\\\\0.4&0.7&-0.8&-1\\\\-1.7&0.7&2.1&0.4 \\end{array}\\right] \\longrightarrow^{\\text{max-pooling}} \\left[\\begin{array}{cc}?&?\\\\?&? \\end{array}\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Local invariance from pooling\n",
    "\n",
    "$$\\displaystyle \\left[\\begin{array}{ccccc} 0&0&255&0&0\\\\0&0&255&0&0\\\\0&0&255&0&0\\\\0&255&0&0&0\\\\255&0&0&0&0 \\end{array}\\right] \\longrightarrow \\left[\\begin{array}{ccc}?&?\\\\ ?&?\\end{array}\\right] \\longleftarrow \\left[\\begin{array}{ccccc} 0&0&255&0&0\\\\0&0&0&0&0\\\\0&255&255&0&0\\\\255&0&0&0&0\\\\0&0&0&0&0\\end{array}\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together\n",
    "\n",
    " * Convolutional neural network alternates between the convolutional and pooling layers\n",
    " \n",
    " \n",
    " \n",
    " * Output is a fully connected layer\n",
    " * .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation for Convolution\n",
    "\n",
    "\n",
    " * Let $l$ be our loss function, and $\\mathbf{y}_j = \\mathbf{x}_i * \\mathbf{w}_{ij}$   \n",
    "   * Gradient of input $\\mathbf{x}_i$\n",
    "   \n",
    "     $$\\frac{\\partial l}{\\partial \\mathbf{x}_i} = \\sum_j \\left(\\frac{\\partial l}{\\partial \\mathbf{y}_j}\\right) \\star \\mathbf{w}_{ij} \\ \\ \\ (\\star\\text{\\equiv correlation})$$  \n",
    "     \n",
    "     $\\star$ represents correlation\n",
    "     \n",
    "   * Gradient of weights $\\mathbf{w}_{ij}$\n",
    "   \n",
    "    $$\\frac{\\partial l}{\\partial \\mathbf{w}_{ij}} = \\sum_j \\left(\\frac{\\partial l}{\\partial \\mathbf{y}_j}\\right) * \\mathbf{x}_{ij}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation for pooling\n",
    "\n",
    "\n",
    "### Gradient of max-pooling\n",
    "\n",
    "$$\\left[\\begin{array}{ccc}x_1&x_2\\\\ x_3&x_4\\end{array}\\right] $$\n",
    "\n",
    "Let's assume that the maximum occurs at $x_2$ for this example., i.e. $y = \\max\\{x_1,x_2,x_3,x_4\\} = x_2$\n",
    "\n",
    "Therefore, we get $\\left[\\begin{array}{ccc}0&1\\\\0&0\\end{array}\\right] $\n",
    "\n",
    "#### Gradient of mean-pooing\n",
    "\n",
    "$\\displaystyle \\left[\\begin{array}{ccc}\\frac{1}{4}&\\frac{1}{4}\\\\ \\frac{1}{4}&\\frac{1}{4}\\end{array}\\right] $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularizing CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Training ensembles\n",
    " * Model averaging\n",
    " * Dropout: randomly drop some hidden units ($a_i$)\n",
    " * DropConnect: randomy drop some of the weights ($\\mathbf{W}_{ij}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0rc4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
