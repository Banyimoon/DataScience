{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimation\n",
    "========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point Estimation\n",
    "\n",
    "Let $X=(X_1, X_2, ..., X_n)$ be iid random variables, and their common distribution depends on a parameter $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Find a function $T$ of the variable $X$ (or perhaps, more correctly of the observed data $x=(x_1,x_2,...x_n)$) to estimate $\\theta$ (to guess what $\\theta$ is).\n",
    "\n",
    "Notation: $\\hat{\\theta}$ or $\\hat{\\theta}(X)$ for this function $T$, if it gives us access to $\\theta$ directly. Otherwise, $T$ could give access to $g(\\theta)$ where $g$ is some function.\n",
    "\n",
    "Idea: as $n\\to \\infty$, we hope $\\hat{\\theta}(X) \\to \\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition:** Mean square error of $\\hat{\\theta}(X)$ is $\\displaystyle \\mathbf{E}_\\theta\\left[(\\hat{\\theta}(X) - \\theta)^2\\right]$ (this $\\mathbf{E}_\\theta$ means evaluate the expectaiton assuming that $\\theta$ is the true parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\displaystyle \\mathbf{E}_\\theta\\left[(\\hat{\\theta}(X) - \\theta)^2\\right] = \\displaystyle \\mathbf{Var}_\\theta\\left[\\hat{\\theta}(X) - \\theta\\right] + \\displaystyle \\left(\\mathbf{E}_\\theta\\left[\\hat{\\theta}(X) - \\theta\\right]\\right)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\displaystyle \\mathbf{E}_\\theta\\left[(\\hat{\\theta}(X) - \\theta)^2\\right] = \\displaystyle \\mathbf{Var}_\\theta\\left[\\hat{\\theta}(X) \\right] + \\left(\\mathbf{E}_\\theta\\left[\\hat{\\theta}(X)\\right] - \\theta \\right)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the MSE of the estimator $\\hat{\\theta}(X)$ is the **sum of two postitive terms**:\n",
    "\n",
    "$$r_{\\hat{\\theta}(X)} (\\theta) = \\mathbf{Var}_\\theta \\left[\\hat{\\theta}(X)\\right] ~+~ \\left(\\mathbf{E}_\\theta \\left[\\hat{\\theta}(X)\\right] - \\theta\\right)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last term (before squared) is called **Bias** :$b_{\\hat{\\theta}}(theta) = \\mathbf{E}_\\theta \\left[\\hat{\\theta}(X)\\right] - \\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition:** When BIAS $b_{\\hat{\\theta}}(\\theta) = 0$ we say $\\hat{\\theta}$ is unbiased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Example:**\n",
    "\n",
    "Estimate $\\theta$ for $X_i \\sim Uniform(0,\\theta)$. We define two different estimators:\n",
    "\n",
    " * $T_1 = \\displaystyle 2 \\frac{X_1 + X_2 + ... + X_n}{n}$ so $\\mathbf{E}[T_1] = \\theta$ (UNBIASED)\n",
    " \n",
    " * $T_2 = \\max \\left(X_1, X_2, ..., X_n\\right)$ turns out $\\mathbf{E}[T_2] \\theta = -\\frac{\\theta}{n + 1}$ (biased)\n",
    " \n",
    "\n",
    "$$r_{T_1}(\\theta) = \\frac{\\theta^2}{3n}$$\n",
    "\n",
    "$$r_{T_2}(\\theta) = \\frac{2 \\theta^2}{(n+1)(n+2)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we see that $r_{T_2}$ is roughly in order of $\\frac{1}{n^2}$ and therefore $T_2$ is better than $T_1$, wven though $T_2$ is biased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________\n",
    "\n",
    "______________________________\n",
    "\n",
    "\n",
    "## Method of Moments\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: for Gamma distribution: $X \\sim \\Gamma(\\alpha, \\theta)$. \n",
    "\n",
    "We know that $\\mathbf{E}[X] = \\alpha \\theta$ and $\\mathbf{Var}[X] = \\alpha \\theta^2$\n",
    "\n",
    "Therefore, if we know $\\mu = \\mathbf{E}[X]$ and $\\sigma^2 = \\mathbf{Var}[X]$. We solve above for $\\alpha$ and $\\theta$\n",
    "\n",
    "\n",
    " * $\\displaystyle \\theta = \\frac{\\sigma^2}{\\mu}$  \n",
    " \n",
    " * $\\displaystyle \\alpha = \\frac{\\mu^2}{\\sigma^2}$  \n",
    " \n",
    "The method of moment estimator is what we get by replacing the $\\mu$ by the data's sample mean $\\displaystyle \\hat{\\mu}_n(X) = \\frac{X_1 + X_2 + ... + X_n}{n}$, and replace $\\sigma^2$ by sample variance $\\displaystyle \\hat{\\sigma}_n^2(X) = \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\hat{\\mu}_n)^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0rc4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
