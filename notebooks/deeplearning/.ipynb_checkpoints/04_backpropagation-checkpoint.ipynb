{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back Propagation\n",
    "====="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log-loss\n",
    "\n",
    "Bernoulli: $P(Y=y|X) = (D)^y \\times (1-D)^{1-y}$\n",
    "\n",
    "$$\\mathbf{E}[\\log P(Y|x)] = \\mathbf{E}[y\\log D + (1-y) \\log(1-D)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* layer pre-activation :\n",
    " * $\\ \\ h^{(0)}(\\mathbf{x}) = x$\n",
    "\n",
    "\n",
    "* Hidden layer activation ($1 \\le k \\le L$):\n",
    "\n",
    "\n",
    "* Output layer activation: \n",
    "\n",
    " $$f(\\mathbf{x})_y = \\frac{\\exp\\left(a^{(L+1)}(\\mathbf{x})_y\\right)}{\\sum_d \\exp\\left(a^{(L+1)}(\\mathbf{x})_d\\right)}$$\n",
    " \n",
    " **Softmax:**\n",
    " \n",
    " \n",
    "Previously, we defind the loss: $$L = \\frac{1}{N} \\sum_{i=1}^{N} l\\left(f(\\mathbf{x}_i,\\theta); y_i\\right)$$\n",
    "\n",
    "And the optimal parameters are $$\\theta^* = arg\\min_\\theta \\frac{1}{N} \\sum_{i=1}^{N} l\\left(f(\\mathbf{x}_i,\\theta); y_i\\right)$$\n",
    "\n",
    "**Solution:** using *gradient decent* to find the optimal parmaters $\\theta^{t+1} = \\theta^t - \\alpha \\nabla l\\left(f(\\mathbf{x}_i,\\theta); y_i\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output layer gradient:\n",
    "\n",
    "Big picture:  we want to find the derivative w.r.t. weights.\n",
    "\n",
    "\n",
    "Strategy: \n",
    "\n",
    "  $$\\frac{\\partial f(x)}{\\partial x}  = \\frac{\\partial f(x)}{\\partial g(x)} \\ \\times\\ \\frac{\\partial g(x)}{\\partial x}$$\n",
    "\n",
    "\n",
    "  * First, we derive $\\frac{\\partial}{\\partial a^{(L+1)}(\\mathbf{x})_c} -\\log f(\\mathbf{x})_y$ for all classes $c=0..(C-1)$. This can be represented as the gradient: $\\nabla_{a^{(L+1)}(\\mathbf{x})} -\\log f(\\mathbf{x})_y$\n",
    "\n",
    "  * \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * **Step 1: derivate w.r.t. $f(\\mathbf{x})$**\n",
    " \n",
    "    * Partial derivative: $$\\frac{\\delta}{\\delta f(\\mathbf{x})_c} -\\log f(\\mathbf{x})_y =-\\frac{1_{y=c}}{f(\\mathbf{x})_y}$$\n",
    "    \n",
    "  * Gradient\n",
    "    $$\\nabla_{f(\\mathbf {x})} -\\log f(\\mathbf{x})_y = \\frac{-1}{f(\\mathbf {x})_y} \\left[\\begin{array}{c}\\mathbf{1}_{y=0}\\\\\\mathbf{1}_{y=1}\\\\ .. \\\\\\mathbf{1}_{y=C-1}\\end{array}\\right] = \\\\ \\frac{-e(y)}{f(\\mathbf {x})_y} \\\\ \\ \\ \\ \\ e(y)\\text{( is called one-shot encoding)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Step2:** \n",
    "  \n",
    "  * Parital form: $$\\frac{\\delta}{\\delta a^{(L+1)}(\\mathbf{x})_c} -\\log f(\\mathbf{x})_y = \\frac{\\delta}{\\delta f(\\mathbf{x})_c} -\\log f(\\mathbf{x})_y \\ \\ \\times \\ \\ $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flow graph\n",
    "\n",
    "### Forward pass\n",
    "\n",
    "* Modular implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Checking\n",
    "\n",
    "### Finite Difference Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0rc4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
