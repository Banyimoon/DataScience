{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review Linear Algebra\n",
    "========\n",
    "\n",
    "Matric Cookbook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector: $x = \\left[x_1 \\ x_2 \\ ... \\ x_n\\right] = \\left[\\begin{array}{c}x_1\\\\x_2\\\\.\\\\.\\\\x_n  \\end{array} \\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Norm: $x^T x = \\|x\\|_2^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Norm of a matrix: $\\sqrt{trace(X^T X)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trace of a mtrix: $\\displaystyle trac(X) = \\sum_i X_{i,i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inverse of a matrix: $X^{-1} X = X X^{-1} = I$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determinant\n",
    " * for a triangular matrix:\n",
    " * for transpose of a mtrix\n",
    " * for inverse of a matrix\n",
    " * for product of a matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties pf matrices \n",
    "\n",
    " * Orthogonal matrix: \n",
    "   * definition $X Y = I$\n",
    "   * if the inverse of $X$ exists, then $X = X^{-1}$\n",
    "   \n",
    " * Positive definite matrix: $v^T X v > 0 \\forall v \\in R$\n",
    "   * if $\\ge$, then its called \"Positive semi-definite\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear dependence\n",
    "\n",
    " * Set of linearly dependent vectors $x_t$\n",
    "    $\\exists w, t^* \\text{such that} x_{t^*} = \\sum_{t \\ne t^*} w_t x_t$\n",
    "    \n",
    " * definition: rank of a matrix is the number of linearly independent min(#column, #row)\n",
    " \n",
    " * definition: range of a matrix\n",
    " \n",
    " $$\\mathcal{R}(X) = \\left\\{x \\in R^n | \\exists w \\text{sucj that} x = \\sum_j w_j X_{.,j} \\right\\}$$\n",
    " \n",
    " * definition: Nullspace of a matrix: $\\left\\{ x \\in R^n | x \\in \\mathcal{R}(X)\\right\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Balue Decomposition\n",
    "\n",
    "$$X = U \\Sigma V^T$$ \n",
    "\n",
    "$U$ spans the row space and $V$ spans the column space of $X$. ???\n",
    "\n",
    "$U^T U = I$\n",
    "$V^TV = I$\n",
    "\n",
    "### Range of a matrix\n",
    "\n",
    "$$\\mathcal{R}(X) = \\{x\\in \\mathbb{R}^n | \\exists w \\text{ such that } x = \\sum_j w_j X_{.,j}\\}$$\n",
    "\n",
    "\n",
    "### Null space:\n",
    "\n",
    "The set of all vectors that results in zero: $A x = 0$ where $x$ is null-space of $A$.\n",
    "\n",
    "$$\\{x\\in \\mathbb{R}^n | x \\notin \\mathcal{R})(X)\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigen Analysis of Matrices\n",
    "\n",
    " * Eigenvalues and eigenvectors (Square matrix)\n",
    "$$\\left\\{ \\lambda_i, u_i| Xu_i = \\lambda_i u_i \\text{ and } u_i^T u_i = 1_{i=j}\\right\\}$$\n",
    "\n",
    " * Properties\n",
    "  * Can write $X = \\sum_i \\lambda_i u_i u_i^T$\n",
    "  * determinant\n",
    "  * positive definite if $\\lambda_i > 0$\n",
    "  * rank is # of non-zero eigen values\n",
    "  \n",
    "#### Singular value decomposition \n",
    "\n",
    " * Applicable to any matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differential Calculus\n",
    "===="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * derivative: $\\displaystyle \\frac{d}{dx}f(x) = \\lim_{\\Delta\\to 0} \\frac{f(x + \\Delta) - f(x)}{\\Delta}$\n",
    " \n",
    " * partial deriviative\n",
    "   * $f(x,y)$\n",
    "   * $$\n",
    "\n",
    "   * Example: $f(x,y) = \\frac{x^2}{y}$\n",
    "     * $\\frac{\\delta}{\\delta x} f(x,y)$\n",
    "     * $\\frac{\\delta}{\\delta y} f(x,y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients\n",
    "\n",
    "\n",
    "### Jacobians, Hesssians\n",
    "\n",
    "\n",
    " * Finding Hessians is in $O(N^3)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taylor Series\n",
    "\n",
    " $$f(x) \\approx f(a) + f'(a)(x-a) + \\frac{1}{2}f''(a)(x-a)^2 + ...$$\n",
    " \n",
    " \n",
    " f(\\vector{x})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * probasbility space: Triplet $\\left( \\Omega, \\mathcal{F}, P\\right)$\n",
    "   * $\\Omega$ is\n",
    "   * \n",
    "   * \n",
    "   \n",
    " * properties of probabilities\n",
    "   * $P(w) \\ge 0 \\forall w \\in \\Omega$\n",
    "   * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributions (joint, marginal, conditional)\n",
    "\n",
    " * joint distribution $P(x_1,..x_n)$\n",
    " * marginal $P(x_1,x_k) = \\displaystyle \\sum_{\\text{all missing variables}}P(x_1,..,x_n)$\n",
    " * conditional $P(x_1|x_2)$\n",
    " \n",
    " $$\\sum_{x_1} P(X_1 = x_1|X_2 = c) = 1$$\n",
    " \n",
    "##### Question: \n",
    "\n",
    "Given two random variables $x_1$ and $x_2$, and having the marginal probabilities $P(x_1),P(x_2)$, can we get their joint distribution $P(x_1,x_2)$?\n",
    "\n",
    "**Answer:** No. We also need to know the conditional distribution: $P(x_1|x_2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain rule, Bayes rule\n",
    "\n",
    " * Probability chain rule $p(s,o) = p(s|o) p(o) = p(o|s)p(s)$\n",
    " \n",
    " $$p(x) = \\prod_i p(x_i|x_1,...x_{i-1})$$\n",
    " \n",
    " \n",
    " * Bayes rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independence between vaiables\n",
    "\n",
    " * \n",
    " \n",
    " * Conditional independence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance matrix\n",
    "\n",
    "\n",
    " * Independetn always implies that $Cov - 0$\n",
    " * If $cov=0$, not always. Only for Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Sampling\n",
    "\n",
    " * generative models\n",
    "\n",
    " * (Q) Have a discrete distribution, we want to sample from this data using the probability distribution.\n",
    "   * (A) generate the cumulative distribution function, then pick a random number between $[0,1)$, and map the random number to discrete sample value\n",
    "   \n",
    " * (A) how about continous case? We have $\\mathbf{p}[x]$ where $x$ is a continous variable.\n",
    "   * (A) the previous method does not work here. For this, let us say we want to estimate some expected value from sampling: $\\mathbf{E}[f(x)] = \\int { f(x) \\mathbf{p}(x)} dx $. We want to estimate the expected value of $f(x)$ using sampling, then with $N$ smaples, the approximation to the expected value will be $\\displaystyle \\mathbf{E}[f(x)] \\approx_{N \\to \\infty} \\frac{1}{N} \\sum_{i=1}^N f(x_i)$\n",
    "\n",
    "### Monte Carlo Eastimate\n",
    "\n",
    " * A method to approximate expensive expectation\n",
    " $$\\mathbf{E}[f(x)] = \\int_x f(x) \\mathbf{p}(x) dx \\approx $$\n",
    " \n",
    "#### Importance sampling\n",
    "\n",
    " $$\\mathbf{E}[f(x)] = \\int_x f(x) \\frac{\\mathbf{p}(x)}{q(X)} q(x) dx \\approx \\frac{1}{K} \\sum_k f(x_k) \\frac{\\mathbf{p}(x)}{q(x)}$$ \n",
    " \n",
    "#### Markov-Chain Monte Carlo (MCMC)\n",
    "\n",
    " * An iterative method to generate the sequence $x^(k)$\n",
    " * selected points are not independent\n",
    " \n",
    " $$x^{(1)} \\rightarrow^{T(x'\\leftarrow x)} x^{(2)} \\rightarrow x^{(3)} ... \\rightarrow x^{(K)}$$\n",
    " \n",
    " * $T(x' \\leftarrow x)$ is a transition operator, that must satify certain properties\n",
    " * usually, we drop the first few samples which are not reliable.\n",
    " \n",
    "#### Gibs Sampling\n",
    " * MCMC method which uses the following transition operator $T(x' \\leftarrow x)$:\n",
    "   * pick a variable $x_i$\n",
    "   * obtain $x'$ by nly resampling this variable according to $\\mathbf{p}(x_i\\vert x_1, x_2, ..x_n)$\n",
    "   * return $x'$\n",
    "   \n",
    "  * Often, we simply cycle through the variables in random order\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Machine Learning\n",
    "\n",
    "### Supervised Learning\n",
    "\n",
    " * Learning example: $(x,y)$\n",
    " \n",
    "### Unsupervised\n",
    "\n",
    " * Clustering\n",
    " * Feature extraction\n",
    " * Dimensionality reduction: learning a lower-dimensional representation of input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Review: Information Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Probability of discrete sample space: $$\n",
    " \n",
    "## Entropy\n",
    "\n",
    " $$H(X) = \\sum_i p_i \\log_2 (\\frac{1}{p_i})$$\n",
    " * Entropy measures \"uncertainty\" of $X$\n",
    " * Entropy has upper bound: $H(X) \\le \\log_2(n)$. Equality holds when $p_i = \\forall i$. (intuitively, it means the outcomes ae equally likely).\n",
    " \n",
    "#### Example: \n",
    "\n",
    " * For a coin toss, we have $p(X=Head)=p,\\ \\ p(X=Tail)=1-p$. Then, the entropy is $\\displaystyle H(X) = p\\log_2(\\frac{1}{p}) + (1-p)\\log_2(\\frac{1}{1-p})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Entropy\n",
    "\n",
    " * Let $X$ and $Y$ be two random variables\n",
    " $$H(x\\vert Y) = \\sum_y H(X\\vert Y=y) Pr(Y=y)$$\n",
    " \n",
    " * $H(X)\\ge H(X\\vert Y)$\n",
    " * If $X$ is independet of $Y$, then $H(X) = H(X\\vert Y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint Entropy\n",
    "\n",
    "$$H(X,Y) = \\sum_{x,y}Pr(X=x, Y=y) \\log_2(\\frac{1}{Pr(X=x, Y=y)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Chain Rule $H(X,Y) = H(X) + H(Y\\vert X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutual Information\n",
    "\n",
    "$$I(X;Y) = H(X)-H(X\\vert Y) \\\\\n",
    "         = H(Y) - H(Y\\vert X) \\\\\n",
    "         = I(Y; X)$$\n",
    "         \n",
    " * $I(X;X) = H(X) - H(X;X) = H(X) $\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Mutual Information\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance Between Distributioins: \n",
    "\n",
    " * Let $p$ and $q$ be two random distributions with the same support (domain)\n",
    "\n",
    "### Total Variation\n",
    "\n",
    " * $D_{TV}(p.q) = \\frac{1}{2} \\|p-q\\|_1 = \\frac{}{2}\\sum |p_i - q_i|$\n",
    " \n",
    "### Hellinger Distance\n",
    "\n",
    " * define:   \n",
    "   $\\sqrt{p} = \\left(\\sqrt{p_1}, ...\\sqrt{p_n}\\right)$\n",
    "   $\\sqrt{q} = \\left(\\sqrt{q_1}, ...\\sqrt{q_n}\\right)$\n",
    " * $\\sqrt{p}$ and $\\sqrt{q}$ are unit vectors\n",
    " \n",
    " $$D_H(p,q) = \\frac{1}{2}\\|\\sqrt{p} - \\sqrt{q}\\|_2$$\n",
    " \n",
    "### Kullback-Leibler Divergence\n",
    "\n",
    " $$D_{KL}(p,q) = \\sum_i p_i \\log_2 \\frac{p_i}{q_i}$$\n",
    " \n",
    " * It's asymmetric distance: $D_{KL}(p,q) \\ne D_{KL}(q,p)$\n",
    " \n",
    "### Jenson-Shannon Distance\n",
    "\n",
    "$$D_{JS}(p,q) = \\frac{D_{KL}(p,r) + D_{KL}(q,r)}{2}$$\n",
    "\n",
    "where $r=\\frac{p+q}{2}$ is the average distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some relations between these distances\n",
    "\n",
    " * $D_{JS}(p,q) \\ge D_H^2(p,q)$\n",
    " \n",
    " * $D_H^2(p,q) \\ge D^2_{TV}(p,q)$\n",
    " \n",
    " * $D_{TV}(p,q) \\ge \\delta$ where $\\delta$ is probibility of distinguishing $p$ from $q$ with a sample.\n",
    " \n",
    " \n",
    " * $TV$ measure is very conservative, while $JS$ is very generous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0rc4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
