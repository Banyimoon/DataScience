{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binomial Distribution\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Efficient way of computing $Binom(k;N,p)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypergeometric Distributions\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have $R$ red balls, and $B$ black balls. Sample $n$ balls without replacement. \n",
    "Let $X=\\#\\text{ of red balls in sample }$.\n",
    "\n",
    "Total number of balls: $N = R + B$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that $$\\mathbf{P}[X = k] = \\frac{\\left(\\begin{array}{c}n\\\\k\\end{array}\\right) \\left(\\begin{array}{c}N-n\\\\R-k\\end{array}\\right)}{\\left(\\begin{array}{c}N\\\\R\\end{array}\\right)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the hypergemoetric distribution. As we mentioned before, $\\mathbf{E}[X] = np$ and $\\mathbf{Var}[X] = np(1-p) \\frac{N-n}{N-1}$ where $p=\\frac{R}{R+B}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: if $n\\ll N$ and $N\\to \\infty$ and $p$ is fixed, then the hypergeometric distribution tends to the binomial distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________\n",
    "\n",
    "For discrete distributions, we say that one distribution converges to another, (as a paramater \"N\" goes to infinity) if the probability mass function of the first one converges to the other's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geometric and Negative Binomial Distributions\n",
    "===="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $r$ be a fixed integer. Let $X_1,X_2,..X_n$ be an infinite sequence of i.i.d. Berboulli random variables with success probability $p$.\n",
    "\n",
    "Let $Y_r$ be the time at which the \"rth\" success occurs.\n",
    "\n",
    "We say that $Y_r$ has the Negative Binomial Distribution with parameters $r$ and $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example: \n",
    "\n",
    "Let $11000110000100$ be a sequence of coin tosses.\n",
    "\n",
    " * $r=5  \\ \\ \\Rightarrow\\ Y_5 = 12$   \n",
    " * $r=2  \\ \\ \\Rightarrow\\ Y_2 = 2$   \n",
    " * $r=3  \\ \\ \\Rightarrow\\ Y_3 = 6$   \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Special case: $r=1$\n",
    "\n",
    "  $Y_1$ is a geometric random variable. Recall $\\mathbf{P}[Y_1 = k] = (1-p)^{k-1} \\ p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What about $\\mathbf{P}[Y_r = k]$?\n",
    "\n",
    "$$\\mathbf{P}[Y_r = k] = \\star \\star \\star (1-p)^{k-r} p^r$$\n",
    "\n",
    "We need to find $\\star \\star \\star$, which is the combination of ways to get $r-1$ sucesses chosen out of $k-1$ slots: $\\left(\\begin{array}{c}k-1\\\\r-1\\end{array}\\right)$\n",
    "\n",
    "Therefore\n",
    "\n",
    "$$\\mathbf{P}[Y_r = k] = \\left(\\begin{array}{c}k-1\\\\r-1\\end{array}\\right)  (1-p)^{k-r} p^r $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: this argument works because for each configuration of successes and failures, the $probability=(1-p)^{k-1}p^r$, and also these configurations are all mutually incompatible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to compute $\\mathbf{E}[Y_r]$ and $\\mathbf{Var}[Y_r]$ efficiently?\n",
    "\n",
    " * Recall that $\\mathbf{E}[Y_1] = \\frac{1}{p}$ and $\\mathbf{Var}[Y_1] = \\frac{1-p}{p^2}$\n",
    "   * Intuition: if the chance of winning is $0.05$, how many times you should expect to play to win for the first time? $\\frac{1}{0.05} = 20$\n",
    " \n",
    " * Then, think of it this way for $r$. It takes $Y_1$ to win the first one. Now, we start fresh and from this time, we \n",
    " \n",
    "  * therefore, $Y_r$ is eseentially the sum of how long it takes to get the first success, and how long it takes from 1st to the second, and ..... and how long it takes to get rth success from (r-1)th success.\n",
    "  \n",
    "  * Since, these times are independent of each other and all have Geometric distribution, $Geom(p)$, we can write \n",
    "  \n",
    "  $$Y_r = X_1 + X_2 + ... X_r\\text{ where }X_i\\text{'s are i.i.d. } \\ Geom(p)$$\n",
    "  \n",
    "  Therefore,\n",
    "  \n",
    "  $$\\mathbf{E}[Y_r] = r\\frac{1}{p}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\mathbf{Var}[Y_r] = r\\frac{1-p}{p^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0rc4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
