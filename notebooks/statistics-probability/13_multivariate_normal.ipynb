{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multivariate Normal $\\chi^2, t, F$\n",
    "========\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Definition:\n",
    "\n",
    " A random vector $X=(X_1, X_2, ..., X_k)^T$ is a multivariate normal if by definition, all linear combination of its components are normal random variables.\n",
    " \n",
    "**Properties: ** \n",
    "\n",
    " (Recall): Let $X\\in\\mathbb{R}^k$.   \n",
    " The density of $X$ at $x$ is \n",
    "           \n",
    "$$f_X(x) = (2\\pi)^{-k/2} det(\\Sigma)^{-1/2} e^{-\\frac{1}{2}\\phi(x)}$$\n",
    "           \n",
    "where $\\Sigma = \\text{covariance of }X$ i.e. $\\Sigma = \\mathbf{Cov}(X_i, X_j)$\n",
    "\n",
    "and $\\phi(x) = (x-\\mu)^T \\Sigma^{-1} (x-\\mu)$ and $\\mu = \\mathbf{E}[X]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Notation:\n",
    "\n",
    "$X\\sim \\mathcal{N_k} (\\mu, \\Sigma)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Let $X$ as above, let $A$ be an $m\\times k$ matrix.     Then, \n",
    "$$AX\\sim \\mathcal{N_m}(A\\mu, A\\Sigma A^T)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * **APPLICATION OF THE PREVIUOUS POINT**\n",
    " \n",
    "Use $A = \\sqrt{\\Sigma}^{-1}$, in other words, pick a matrix $A$ with this property: $AA^t = A^TA = \\Sigma^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, construct $Y = AX - \\mu$.\n",
    "\n",
    "Easy to prove $Y \\sim \\mathcal{N_k}(0, I_k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: this about $Y$ is true if and only if $Y$'s components are $\\mathcal{N}(0,1)$ iid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example\n",
    "\n",
    "Easy calculation: let $t = (t_1, t_2, ..., t_k)$.\n",
    "\n",
    "Let $Z=t_1 X_1 + t_2 X_2 + ... t_k X_k$\n",
    "\n",
    "By definition, $Z$ is normal, $\\mathbf{E}[X] = \\sum_{i=1}^k t_i \\mu_i = t\\mu \\text{ (vector notation) }$\n",
    "\n",
    "$$\\mathbf{Cov}(Z) = \\sum_{i=1}^k\\sum_{j=1}^k t_i t_j \\Sigma_{ij} = t\\Sigma t^T \\text{vector notation} \\\\ \\text{  where } \\Sigma_{ij} = \\mathbf{E}[(X_i-\\mu_i)(X_j - \\mu_j)] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, $Z \\sim \\mathcal{N}(t\\mu, t\\Sigma t^T)$\n",
    "\n",
    "Consequently, $$\\mathbf{E}[e^{t_1X_1 + t_2X_2 + ... + t_nX_n}] = \\displaystyle  e^{\\displaystyle t\\mu + \\frac{1}{2} t~\\Sigma ~t^T} = \\text{ MGF of multivariate normal }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * In particular, two normal random vectors $X_1,X_2$ with dimensions $k_1,k_2$ are independent of each other if and only if their covariance matrix of $X=\\left(\\begin{array}{c}X_1\\\\X_2\\end{array}\\right)$ is of the block form \n",
    " \n",
    " $$\\left(\\begin{array}{cccccc} & \\Sigma_{X_1} & & 0 & &\\\\ & & 0 & & \\Sigma_{X_2} &\\end{array}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Application:\n",
    "\n",
    "Let $Y_1,Y_2,.. Y_n$ be iid $\\sim \\mathcal{N}(\\mu, \\sigma^2)$.\n",
    "\n",
    "Let $X_i = Y_i - \\bar{Y}$ where $\\bar{Y}=  \\frac{1}{n} \\sum_{i=1}^n Y_i$\n",
    "\n",
    "These $X_i$'s are the deviations of the $X$'s from the sample mean.\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$\\mathbf{Cov}(\\bar{Y}, X_i) = \\mathbf{Cov}(\\bar{Y}, Y_i - \\bar{Y}) = \\mathbf{Cov}(\\bar{Y}, Y_i) - \\mathbf{Cov}(\\bar{Y},\\bar{Y})$$\n",
    "\n",
    "We can see that $\\mathbf{Cov}(\\bar{Y},\\bar{Y}) = \\mathbf{Var}[\\bar{Y}] = \\frac{\\sigma^2}{n}$\n",
    "\n",
    "\n",
    "Also, $\\mathbf{Cov}(\\bar{Y}, Y_i) = \\frac{1}{n}\\mathbf{Var}[Y_i] = \\frac{\\sigma^2}{n}$\n",
    "\n",
    "Therefore, $$\\mathbf{Cov}(\\bar{Y}, X_i) = 0$$\n",
    "\n",
    "The covariance calculation shows that $\\bar{Y}$ and $X_i$ are uncorrelated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But since $X$ is a Normal vector, by definition, $(\\bar{Y}, X_i)$ is a 2-dimensional normal vector. So, since everyone is jointly normal, $\\bar{Y},X_i$ are independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\chi^2$ Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $Z_i \\sim ~iid ~\\mathcal{N}(0,1)$\n",
    "\n",
    "Let $Q = \\sum_{i=1}^n Z_i^2$\n",
    "\n",
    "Then, $$Q\\sim \\chi^2 ~\\text{ with n degrees of freedom}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $X_i \\sim ~iid~\\mathcal{N}(0,\\sigma^2)$\n",
    "\n",
    "Let $\\bar{X} = \\frac{1}{n} \\sum_{}X_i$ (sample mean)\n",
    "\n",
    "Let $S^2 = \\frac{1}{n-1}\\sum_{} (X_i-\\bar{X})^2$ (sample variance)\n",
    "\n",
    "Then, $$S^2 \\text{ and } \\bar{X}\\text{ are independent}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason for that is because of the previous example we just did.\n",
    "\n",
    "And, moreover, \n",
    "\n",
    "Let $W = \\frac{S^2 (n-1)}{\\sigma^2}$, we get $W\\sim \\chi^2_{n-1}$ (with $n-1$ degrees of freedom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0rc4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
