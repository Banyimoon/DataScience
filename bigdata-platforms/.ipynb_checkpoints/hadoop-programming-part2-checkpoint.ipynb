{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop Framework for Data Processing\n",
    "====\n",
    "\n",
    "\n",
    "#### Outline\n",
    " * **Data processing in Hadoop**\n",
    "   * Mapper, reducer, and partitioners\n",
    " * **Hadoop I/O**\n",
    "   * InputFormat and OutputFormat\n",
    "   * Readin sequence files\n",
    "   * Compressing the output of mappers and reducers\n",
    " * **Chaining multiple map and reduce tasks**\n",
    " \n",
    "## Hadoop Processing\n",
    "\n",
    "Hadoop splits the input data among the mappers. Each mapper will generate (key,value) pairs, and the intermediate output of mappers is partitioned for the reducers and the partitions are written into disk (local disk to each mapper, not HDFS). \n",
    "\n",
    "#### Partitioner\n",
    "When there are multiple reducers, the output of mappers need to be partitioned. There is a default partitioner that creates partitions by hashing the mappers' output keys:\n",
    "\n",
    "```java\n",
    "job.setPartitionerClass(HashPartitioner.class);\n",
    "\n",
    "public class HashPartitioner<K,V> extends Partitioner<K,V> {\n",
    "    public int getPartition(K ket, V value, int numReduceTasks) {\n",
    "        return (key.hashcode() & Integer.MAX_VALUE) % numReduceTasks;\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example wordcount.java\n",
    "\n",
    "The general scheme of a java program in hadoop is implemented in a main class, which the main class has a mapper class, a reducer class and a main function, as follows\n",
    "\n",
    "<img src=\"hadoop-program.png\" width=200></img>\n",
    "\n",
    "```java\n",
    "public class wordcount {\n",
    "    public class TokenizerMapper extends Mapper<Object, Text, Text, IntWritable> {...}\n",
    "    \n",
    "    public class IntSumReducer extends Reducer<Text, IntWritable, Text, IntWritable> {...}\n",
    "    \n",
    "    public static void main(string[] args) throws Exception {\n",
    "       ...\n",
    "       ...\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InputFormat (only for mappers)\n",
    "\n",
    "InputFormat is an abstract class to specify the way input records are defined in input data. By default, the input format is TextInputFormat where each record is a line and the input key is byte offset, and valye is contetn of the line.\n",
    "\n",
    "```java\n",
    "public abstract class InputFormat<K,V> {\n",
    "    public abstract List<inputSplit> getSplit(JobContext context) throws Exception;\n",
    "    \n",
    "    public abstract RecordReader<K,V> {\n",
    "        createRecordReader(inputSplit split,\n",
    "                           TaskAttempt(Context context)) \n",
    "                           throws Exception;\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "| **InputFormat** | **Description** |\n",
    "|:--:|:--|\n",
    "|TextInputFormat|Each line is a record; <br> *key:LongWritable*<br> *value:Text*|\n",
    "|KeyValueTextInputFormat||\n",
    "|SequenceFileInputFormat||\n",
    "|NLineInputFormat||\n",
    "\n",
    "Specifying InputFormat:\n",
    "```java\n",
    "public static void main(String[] args) throws Exception {\n",
    "    Job job = new Job();\n",
    "    ...\n",
    "    job,setInputFormatClass(TextInputFormat.class);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OutputFormat (only for reducers)\n",
    "\n",
    "OutputFormat is an abstract class to determine the format of purput from reducers. To set the OutputFormat class use **`setOutputFormatClass()`**. The possible options are\n",
    "\n",
    "|**OutputFormat** | **Description** |\n",
    "|:--:|:--|\n",
    "|TextOutputFormat< K , V >||\n",
    "|SequenceFileOutputFormat< K, V >||\n",
    "|NullOutputFormat< K, V>||\n",
    "\n",
    "##### Example\n",
    "\n",
    "```java\n",
    "public static void main(String[] args) throws Exception {\n",
    "    Job job = new Job();\n",
    "    ...\n",
    "    job,setOutputFormatClass(TextOutputFormat.class);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compressing the outputs (for mappers and reducers)\n",
    "\n",
    "If the input data is compressed, hadoop can automatically handle compressed inpit file with no modifications needed. An example is to run hadoop with the compressed input file, while the same Java program could be used for both compressed or uncompressed input:\n",
    "\n",
    "```bash\n",
    "# Running with uncompressed input file\n",
    "hadoop jar wordcount.jar WordCount /user/hduser/wordcount/bigdata.txt /user/hduser/wordcount/output\n",
    "\n",
    "# Running with compressed input file\n",
    "hadoop jar wordcount.jar WordCount /user/hduser/wordcount/bigdata.txt.gz /user/hduser/wordcount/output\n",
    "```\n",
    "\n",
    "However, sometimes it is desirable to compress ..\n",
    "\n",
    "Various compression formats are available, such as \\*.gz, \\*.bz2, etc). Hadoop uses some implementation of compression called ***CompressionCodec*** as an interface to compress the output. Some codecs are given below\n",
    "\n",
    "|Format | Splittable? | HadoopCompressionCodec|\n",
    "|:--|:--|:--|\n",
    "|DEFLATE|No|org.apache.hadoop.io.compress.DefaultCodec|\n",
    "|gzip|No|org.apache.hadoop.io.compress.GzipCodec|\n",
    "|bzip2|Yes|org.apache.hadoop.io.compress.BZip2Codec|\n",
    "|LZO|No|org.apache.hadoop.io.compress.LzoCodec|\n",
    "|Snappy|No|org.apache.hadoop.io.compress.SnappyCodec|\n",
    "\n",
    "##### Example: reducer output compression\n",
    "\n",
    "```java\n",
    "public static void main(String[] args) throws Exception {\n",
    "    Job job = new Job();\n",
    "    ...\n",
    "    FileOutputFormat.setCompressOutput(job, true);\n",
    "    FileOutputFormat.setOutputCompressorClass(BZip2Codec.class);\n",
    "}\n",
    "```\n",
    "which makes the following output file: `part-r-00000.bz2`.\n",
    "\n",
    "#### Mapper output compression\n",
    "\n",
    "```java\n",
    "public static void main(String[] args) throws Exception {\n",
    "    Configuration conf = new Configuration();\n",
    "    conf.setBoolean(\"mapred.compress.map.output\", true);\n",
    "    conf.setClass(\"mapred.map.output.compression.class\", BZip2Codec.class, CompressionCodec.class);\n",
    "    ...\n",
    "    Job job = new Job();\n",
    "    ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chaining\n",
    "\n",
    "Sometimes it is necessary to chan multiple mappers and reducers to accomplish complex problems. Chaining involves sequentially calling mappers and reducers one after another. The input to next job, is the output of current one. This is accomplished by ***ChainMapper*** and ***ChainReducer***. \n",
    "\n",
    "\n",
    "##### Example: ChainMapper and ChainReducer\n",
    "```java\n",
    "Configuration conf = getConf();\n",
    "JobConf job = new JobConf(conf);\n",
    "\n",
    "job.setJobName(\"A-chain-job\");\n",
    "job.setInputFormat(TextInputFormat.class);\n",
    "job.setOutputFormat(TextOutputFormat.class);\n",
    "\n",
    "FileInputFormat.setInputPaths(job, in);\n",
    "FileOutputFormat.setPutputPath(job, out);\n",
    "\n",
    "// Mapper A (preprocessing)\n",
    "JobConf mapAconf = new JobConf(false);\n",
    "ChainMapper.addMapper(job, MapA.class, \n",
    "                        LongWritable.class, Text.class, \n",
    "                        Text.class, Text.class,\n",
    "                        true, mapAconf);\n",
    "\n",
    "// Mapper B (preprocessing)\n",
    "JobConf mapBconf = new JobConf(false);\n",
    "ChainMapper.addMapper(job, MapB.class, \n",
    "                      Tex.class, Text.class,\n",
    "                      LongWritable.class, Text.class, \n",
    "                      true, mapBconf);\n",
    "\n",
    "// Reducer\n",
    "JobConf reduceConf = new JobConf(false);\n",
    "ChainReducer.addReducer(job, myReducer.class, \n",
    "                        LongWritable.class, Tex.class,\n",
    "                        Text.class, Text.class,\n",
    "                        true, reducer1conf);\n",
    "\n",
    "// Mapper C (post-processing)\n",
    "JobConf mapCconf new JobConf(false);\n",
    "ChainReducer.addMapper(job, MapC.class,\n",
    "                       Text.class, Text.class,\n",
    "                       LongWritable.class, Text.class,\n",
    "                       true, mapCconf);\n",
    "\n",
    "// Mapper D (post-processing)\n",
    "JobConf mapDconf new JobConf(false);\n",
    "ChainReducer.addMapper(job, MapD.class,\n",
    "                       LongWritable.class, Text.class,\n",
    "                       LongWritable.class, Text.class,\n",
    "                       true, mapDconf);\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
