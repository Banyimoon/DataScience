{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop Programming\n",
    "====\n",
    "\n",
    "\n",
    "```\n",
    "Usage: hadoop [--config confdir] [COMMAND] [GENERIC_OPTIONS] [COMMAND_OPTIONS]\n",
    "```\n",
    "\n",
    " * **Commands** \n",
    " \n",
    "|Command | Description|\n",
    "|:--|:--|\n",
    "|archive|create a Hadoop archive|\n",
    "|jar | to run a jar file|\n",
    "|classpath\n",
    "|distcp||\n",
    "|fs||\n",
    "|version||\n",
    "\n",
    "\n",
    " * **Generic options:**\n",
    " \n",
    "|Generic Option | Description |\n",
    "|:---|:---|\n",
    "|`-conf <config file>`|Specify the config file|\n",
    "|`-D <propert=value>`|Set the value of a property|\n",
    "|`-fs <local> or <namenode:port>```| specify a namenode|\n",
    "|`-jt <local> or <jobtracker:port>` | |\n",
    "|`-files <comma separated list of files>`|Specify  files to be copied to the map reduce cluster|\n",
    "|`-libjars <comma seperated list of jars>`|Specify jar files to include in the classpath|\n",
    "|`-archives <comma separated list of archives>`||\n",
    "\n",
    "## Writing a Mapper Class\n",
    "\n",
    "* **Context** objects are used to write the output of map() function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Hadoop Data Types for Keys & Values\n",
    "\n",
    "* **WritableComparable** can be used for both keys & values\n",
    "* **Writable** interface is for efficiently serializing objects for input and output. can be used for values\n",
    "\n",
    "|Class|Description|\n",
    "|:---:|:----:|\n",
    "|BooleanWritable|Standard boolean writable|\n",
    "|ByteWritable|a single byte|\n",
    "|DoubleWritable|a double|\n",
    "|FloatWritable|a float|\n",
    "|IntWritable|an integer|\n",
    "|LongWritable|a long|\n",
    "|Text|to store text using UTF8 format|\n",
    "|NullWritable|Placeholder when the key or value is not needed|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimial Hadoop Program\n",
    "\n",
    "The most simple Hadoop program one can write must contain the minimial components of a Hadoop program. Minimial Hadoop program uses the default mapper, i.e. Mapper.class, and the default reducer Reducer.class\n",
    "\n",
    " * Mapper.class reads record line by line. The key is the offset from begining of line (LongWritable) and value is Text\n",
    " * The default reducer (Reducer.class ) writes its input directly to output as is, without any aggregation\n",
    " \n",
    "**miminal_hadoop.java**\n",
    "```java\n",
    "import java.io.IOException;\n",
    "import org.apache.hadoop.fs.Path;\n",
    "import org.apache.hadoop.conf.Configuration;\n",
    "import org.apache.hadoop.mapreduce.Job;\n",
    "import org.apache.hadoop.mapreduce.MRJobConfig;\n",
    "import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n",
    "import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n",
    "\n",
    "public class MinimalProgram {\n",
    "    public static void main(String[] args) throws Exception {\n",
    "        Configuration conf = new Configuration();\n",
    "        Job job = Job.getInstance(conf, \"minimal program\");\n",
    "        job.setJarByClass(minimal.class);\n",
    "        FileInputFormat.addInputPath(job, new Path(args[0]));\n",
    "        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n",
    "        System.exit(job.waitForCompletion(true) ? 0 : 1);\n",
    "    }\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    " * Compile\n",
    "\n",
    "```bash\n",
    "hduser@ubuntu$ javac -cp [provide class path] minimal.java\n",
    "```\n",
    "\n",
    " * Archive into a jar file\n",
    " \n",
    "```bash\n",
    "hduser@ubuntu$ jar cvf minimal.jar *.class\n",
    "```\n",
    "\n",
    " * Run Hadoop\n",
    "\n",
    "```bash\n",
    "hduser@ubuntu$ hadoop jar minimal.jar MinimalProgram /example/input.txt /example/out\n",
    "```\n",
    "\n",
    " * Retrieve the results from HDFS\n",
    " \n",
    "```bash\n",
    "hduser@ubuntu$ hadoop dfs -cat /example/out/part-r-00000\n",
    "0\tMaster Kenobi, you disappoint me.\n",
    "34\tYoda holds you in such high esteem.\n",
    "70\tSurely you can do better!\n",
    "```\n",
    "\n",
    "#### Understanding components of this minimal Hadoop program\n",
    "\n",
    " * **Libraries**\n",
    " * **Configurations**\n",
    " * **Job setup**\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop Program Configuraions\n",
    "\n",
    " * **Default properties** mapred-default.xml\n",
    " * **Specifying properties in a configuration directory** \n",
    " \n",
    " ```hadoop --config <config_dir> jar <jarfile> <class_name>```\n",
    " \n",
    " * **Specify the configurations explicitly within the program**\n",
    " \n",
    " ```java\n",
    " Configuration conf = new Configuration();\n",
    " conf.set(\"property1\", \"value1\");\n",
    " ```\n",
    " * Modify the configuration properties with generic options \n",
    " \n",
    " ```\n",
    " hadoop jar <jarfile> <class_name> -D property=value\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordcount\n",
    "\n",
    "\n",
    "**wordcount_v1.java**\n",
    "```java\n",
    "import java.io.IOException;\n",
    "import org.apache.hadoop.conf.*;\n",
    "import org.apache.hadoop.fs.Path;\n",
    "import org.apache.hadoop.mapreduce.Job;\n",
    "import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n",
    "import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n",
    "import org.apache.hadoop.util.Tool;\n",
    "import org.apache.hadoop.util.ToolRunner;\n",
    "\n",
    "\n",
    "import org.apache.hadoop.mapreduce.lib.map.TokenCounterMapper;\n",
    "import org.apache.hadoop.mapreduce.lib.reduce.IntSumReducer;\n",
    "\n",
    "import org.apache.hadoop.io.Text;\n",
    "import org.apache.hadoop.io.IntWritable;\n",
    "\n",
    "public class wordcount extends Configured implements Tool {\n",
    "\n",
    "  public int run(String[] args) throws Exception {\n",
    "    Job job = new Job(getConf());\n",
    "    job.setJarByClass(getClass());\n",
    "    job.setMapperClass(TokenCounterMapper.class);\n",
    "    job.setReducerClass(IntSumReducer.class);\n",
    "\n",
    "    job.setOutputKeyClass(Text.class);\n",
    "    job.setOutputValueClass(IntWritable.class);\n",
    "\n",
    "    FileInputFormat.addInputPath(job, new Path(args[0]));\n",
    "    FileOutputFormat.setOutputPath(job, new Path(args[1]));\n",
    "    return job.waitForCompletion(true) ? 0 : 1;\n",
    "  }\n",
    "\n",
    "  public static void main(String [] args) throws Exception {\n",
    "    int exitCode = ToolRunner.run(new wordcount(), args);\n",
    "    System.exit(exitCode);\n",
    "  }\n",
    "\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "**Built-in Mapper and Reducers:**\n",
    "\n",
    "  * TokenCounterMapper  \n",
    "  ```java\n",
    "  import org.apache.hadoop.mapreduce.lib.map.TokenCounterMapper;\n",
    "  ```\n",
    "  \n",
    "  * IntSumReducer \n",
    "  ```java\n",
    "  import org.apache.hadoop.mapreduce.lib.reduce.IntSumReducer;\n",
    "  ```\n",
    "  \n",
    "  * Set the types of output keys and values of reducer\n",
    "  ```java\n",
    "    job.setOutputKeyClass(Text.class);\n",
    "    job.setOutputValueClass(IntWritable.class);\n",
    "  ```\n",
    "  \n",
    "Put everything together\n",
    "  \n",
    "```java\n",
    "import java.io.IOException;\n",
    "import org.apache.hadoop.conf.*;\n",
    "import org.apache.hadoop.fs.Path;\n",
    "import org.apache.hadoop.mapreduce.Job;\n",
    "import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n",
    "import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n",
    "import org.apache.hadoop.util.Tool;\n",
    "import org.apache.hadoop.util.ToolRunner;\n",
    "\n",
    "import org.apache.hadoop.mapreduce.lib.map.TokenCounterMapper;\n",
    "import org.apache.hadoop.mapreduce.lib.reduce.IntSumReducer;\n",
    "\n",
    "import org.apache.hadoop.io.Text;\n",
    "import org.apache.hadoop.io.IntWritable;\n",
    "\n",
    "public class wordcount extends Configured implements Tool {\n",
    "\n",
    "  public int run(String[] args) throws Exception {\n",
    "    Job job = new Job(getConf());\n",
    "    job.setJarByClass(getClass());\n",
    "    job.setMapperClass(TokenCounterMapper.class);\n",
    "    job.setReducerClass(IntSumReducer.class);\n",
    "\n",
    "    job.setOutputKeyClass(Text.class);\n",
    "    job.setOutputValueClass(IntWritable.class);\n",
    "\n",
    "    FileInputFormat.addInputPath(job, new Path(args[0]));\n",
    "    FileOutputFormat.setOutputPath(job, new Path(args[1]));\n",
    "    return job.waitForCompletion(true) ? 0 : 1;\n",
    "  }\n",
    "\n",
    "  public static void main(String [] args) throws Exception {\n",
    "    int exitCode = ToolRunner.run(new wordcount(), args);\n",
    "    System.exit(exitCode);\n",
    "  }\n",
    "\n",
    "}\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
