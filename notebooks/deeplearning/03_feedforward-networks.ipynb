{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed-Forward Networks\n",
    "====="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Learning in Neural Networks\n",
    "\n",
    "\n",
    " * based on \"Empricial risk minimization\"\n",
    " \n",
    "$$arg \\min_\\theta \\frac{1}{T} l(f(x^t;\\theta), y^t) + \\lambda \\Omega (\\theta)$$\n",
    "\n",
    " * Cost funciton: $l(f(x^t;\\theta), y^t)$ (also called risk)\n",
    " * Regularizer: $\\Omega (\\theta)$\n",
    " * $\\lambda$ is a trade-off between the two terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * NN estimates $f(x)_c = p(y=c\\vert x)$\n",
    "  * we want to maximize $\\displaystyle \\theta^* = arg \\max_\\theta \\frac{1}{T} \\prod_t P(y=c\\vert x_t) + \\lambda \\Omega (\\theta)$\n",
    "  * If we have $\\prod_t P(y=c\\vert x_t)$ it can be re-written as $arg \\max_\\theta \\frac{1}{T} \\sum_t \\log P(y=c\\vert x_t)$. Since $\\log$ is a monotonic increasing funciton.\n",
    "  * another variation: $\\displaystyle \\theta^* =arg \\min \\left(- \\sum_t \\log P(y=c\\vert x_t)\\right)$\n",
    "  \n",
    "  * Numerical instability: since $P(..)$ are small numbers, mulltiplications of such small numbers many times will cause loosing the precision. $\\log$ computation will help with nmerical stability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions for classification\n",
    "\n",
    " * $0-1$ loss: $l()=\\sum_i \\mathcal{I}(\\hat{y}_i = y_i)$\n",
    "   * The most ideal, but not practical for optimization\n",
    "   \n",
    "   \n",
    " \n",
    " * Surrogate loss functions\n",
    "   * Squared loss: $(y-f(x))^2$ (bad loss function, but still used)\n",
    "   * Logistic loss function $\\log \\left(1 + e^{-y f(x)}\\right)$\n",
    "   * Hinge loss: $(1 - yf(x))_+$\n",
    "   * Squared Hinge loss: $(1-yf(x)))^2_+$\n",
    "   \n",
    "Notation: $[x]_+ = \\max (0, x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Loss functions for regression\n",
    "\n",
    " * Euclidean Loss: $\\|y - f(x)\\|_2^2$\n",
    " * Manhattan Loss: $\\|y - f(x)\\|_1$\n",
    " \n",
    "   * Manhattan loss is more aggressive than Euclidean for values close to zero\n",
    "   * But less aggressive for larger values\n",
    " * Huber Loss: $\\left\\{\\begin{array}{lr} 1/2\\|y-f(x)\\|_2^2 & for \\|y-f(x)\\|_2^2 \\| \\delta^2\\\\ \\delta \\|y-f(x)\\|_1 - 1/\\delta^2 & \\text{otherwise}\\end{array} \\right.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** In classification, error (risk/missclassification) is determined by product $y_i f(x_i)$. But in regression, it is determined via the difference $y_i-f(x_i)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0rc4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
