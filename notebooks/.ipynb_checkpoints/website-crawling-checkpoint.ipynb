{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gathering Data from Web\n",
    "===\n",
    "\n",
    "## Website Crawler (spider)\n",
    "\n",
    "Automatically traverses the hyperlinks in world wide web, ad gather web pages.\n",
    "\n",
    "**Snoaball Sampling:** Start from some seed URLs and reversively extract hyperlinks to other URLs.\n",
    "\n",
    "Restrictions to consider:\n",
    " 1. In order to avoid overloading the servers, website admins usually create a guideline file named \"robot.txt\" that specifies some restrictions for that website.\n",
    " 2. Meta tags also conveys restrictions to a crawler   \n",
    " Example: <META NAME=\"ROBOTS\", CONTENT=\"NOINDEX\">\n",
    " \n",
    " NOINDEX: to no appear in Google's index  \n",
    " NOFOLLOW: to not follow links on this page  \n",
    " NOARCHIVE: to not archive on search results\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wget\n",
    "\n",
    "Wget is a free GNU software for crawling and retrieving data from HTTP, HTTPS, and FTP over internet.\n",
    "\n",
    "Wget supports\n",
    " * recursively traverse HTML documents and FTP directory trees\n",
    " * specify wildcards to match certain files\n",
    " * restrict the max depth of directory traversal\n",
    " \n",
    "##### Example\n",
    "\n",
    "* Retrieve the index.html file from www.vahidmirjalili.com, and retry 20 times if access fails.\n",
    "```bash\n",
    "wget -t 20 http://vahidmirjalili.com\n",
    "```\n",
    "\n",
    "* Recursively retrieve files (default depth=4)\n",
    "```bash\n",
    "wget -r http://vahidmirjalili.com\n",
    "```\n",
    "\n",
    "##### wget options\n",
    "\n",
    "\n",
    "```bash\n",
    "-O                         specify output file\n",
    "--limit-rate=200k          specify the doanload speed\n",
    "-b                         download in background\n",
    "--user-agent=\"Mozilla/..\"  display wget as a browser\n",
    "-i list-of-urls.txt        download multiple URLs listed in input file\n",
    "--mirror                   turn on mirror options \n",
    "-p                         download full website\n",
    "--convert-links            Convert links to allow local viewing\n",
    "```\n",
    "\n",
    "Download full website to be viewed locally:\n",
    "\n",
    "```bash\n",
    "wget --mirror -p --convert-links http://vahidmirjalili.com\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieiving Data Using API\n",
    "\n",
    " * Twitter API: https://dev.twitter.com\n",
    " * Facebook:    http://devlopers.facebook.com\n",
    " * Reddit:      https://github.com/reddit/reddit/wiki/API\n",
    " \n",
    "Format of Data Returned From APIs:\n",
    " * JSON (Javascript Object Notation)\n",
    " * XML (Extensible Markup Language)\n",
    " \n",
    "\n",
    "**Encoding Data into JSON in PHP:**\n",
    "\n",
    "Using array:\n",
    "\n",
    "```html\n",
    "<?php\n",
    "   $book = array(\"code\" => \"DS110\",\n",
    "                 \"title\" => \"Elements of Data Science\",\n",
    "                 \"Year\" => \"2016\");\n",
    "                 \n",
    "   echo json_encode($book);\n",
    "?>\n",
    "```\n",
    "\n",
    "Using class:\n",
    "```html\n",
    "<?php\n",
    "   class Book {\n",
    "       public $code = \"\";\n",
    "       public $title = \"\";\n",
    "       public $year = \"\";\n",
    "   }\n",
    "   \n",
    "   $b = new Course();\n",
    "   $b->code = \"DS110\";\n",
    "   $b->title = \"Elements of Data Science\";\n",
    "   $b->year = \"2016\";\n",
    "   \n",
    "   echo json_encode($book);\n",
    "?>\n",
    "```\n",
    "\n",
    "**Decoding Data into JSON in PHP:**\n",
    "\n",
    "```html\n",
    "<?php\n",
    "   $myjson = '{\"a\":\"1\", \"b\":\"2\", \"c\":\"3\"}';\n",
    "   $arr = json_decode($myjson, true);\n",
    "   \n",
    "   echo $arr['a'].\" \".$arr['b'].\"<BR>\";\n",
    "?>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter API\n",
    "\n",
    "  * **Streaming API**\n",
    "\n",
    "  * **Search (REST) API**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python Twitter Seacrh API\n",
    "\n",
    "```python\n",
    "import tweepy\n",
    "import sys\n",
    "\n",
    "C_KEY    = 'XXXXX'\n",
    "C_SECRET = 'XXXXX'\n",
    "ACCESS_TOKEN_KEY = 'XXXXX'\n",
    "ACCESS_TOKEN_SECRET = 'XXXXX'\n",
    "\n",
    "# Authentication\n",
    "auth = tweepy.OAuthHandler(C_KEY, C_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN_KEY, ACCESS_TOKEN_SECRET)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "# Search API\n",
    "if (len(sys.argv)==1):\n",
    "    print(\"Please provide a keyword to search\")\n",
    "else\n",
    "    posts = api.search(sys.argv[1], rpp=15)\n",
    "    for tweet in posts:\n",
    "        print(\" \" + str(tweet.text.encode(\"UTF-8\")))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
