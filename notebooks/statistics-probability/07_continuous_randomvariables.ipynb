{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continous Random variables\n",
    "==========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Recall the PMF for a **discrete** random variable $X$ for any $x\\in \\mathbb{Z}$: $p_X(x) = \\mathbf{P}[X=x]$   \n",
    " Then, we can write $$\\mathbf{P}[n \\le X \\le m] = \\sum_{i=n}^m p_X(i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition:\n",
    "\n",
    "We say that a random variable $X$ is continous and has density $f_X$ if the probability $\\displaystyle \\mathbf{P}[a \\le X \\le b] = \\int_a^b f_X(x) dx$\n",
    "\n",
    "\n",
    "(This is because of Riemann's sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, ***symbolically***: $\\displaystyle \\mathbf{P}[x\\le X\\le x+dx] = f_X(x) dx$\n",
    "\n",
    "This shows that $f_X(x)$ by itself is not a probability, but rather some sort of *probability intensity*. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition:**\n",
    "\n",
    "If $X$ is a random variable, (discrete, continous, or otherwise), we let $F_X(x) = \\mathbf{P}[X\\le x]$ for any $x\\in \\mathbb{R}$.   \n",
    "This $F_X(x)$ is called the **cumulative distribution function** of $X$. \n",
    "\n",
    "Note: if $X$ has density $f_X$, then $\\displaystyle F_X(x) = \\int_{-\\infty}^x f_X(x) dx$\n",
    "\n",
    "This is from the definition od $f_X$ with $a=-\\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In this case, we also have $\\displaystyle \\frac{dF_X}{dx} f_X(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This follows directly from the so-called \"fundamental theorem of Calculus\".\n",
    "\n",
    "Some facts: \n",
    " * Also, when $X$ has a density, then $F_X$ is continous.\n",
    "\n",
    " * $\\displaystyle \\int_{-\\infty}^\\infty f(x) dx = 1$     \n",
    " \n",
    " * $f_X(x) \\ge 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Properties:**\n",
    "\n",
    "\n",
    " \n",
    " * $F_X$ increases from $0$ to $1$   \n",
    " \n",
    " * The limits $0$ and $1$ at $-\\infty$ and $+\\infty$ exists because of \"monotonous convergence of probabilities\".   \n",
    " \n",
    " * For $X$ with density $f_X$, the value of $f_X$ at a single point or even a countable collection of points does not matter.   \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Examples\n",
    "\n",
    "  * Example 1: (Fundamental): Uniform density on $[0,1]$. $f_X(x) = \\left\\{\\begin{array}{lr}1 & \\text{if }x\\in[0,1] \\\\ 0 & \\text{otherwise}\\end{array}\\right.$    \n",
    "  \n",
    "  * Example 2: $X$ with density $f_X(x) = \\left\\{\\begin{array}{lr}constant & \\text{if }x\\in [0,1] \\\\ 0 & \\text{otherwise}\\end{array}\\right.$      \n",
    "    We conclude that $\\int_{-\\infty}^\\infty f_X(x) dx = 1 \\ \\ \\Longrightarrow constant = 2$\n",
    "    \n",
    "  * Example 3:  $X$ with density $f_X(x) = \\left\\{\\begin{array}{lr}constant & \\text{if }x\\in [a,b] \\\\ 0 & \\text{otherwise}\\end{array}\\right.$     \n",
    "     Then the constant is $\\displaystyle constant = \\frac{1}{b-a}$   \n",
    "\n",
    "  * Example 4: $X$ with density $\\displaystyle f_X(x) = \\left\\{\\begin{array}{lr}\\displaystyle \\frac{1}{2}x & \\text{if }x\\in [0,2] \\\\ 0 & \\text{otherwise}\\end{array}\\right.$   \n",
    "  \n",
    "  * Example 5: (fundamental) $X$ with density $\\displaystyle f_X(x) = \\left\\{\\begin{array}{lr}\\displaystyle \\lambda e^{-\\lambda x} & \\text{for }x > 0 \\\\ 0 & \\text{otherwise}\\end{array}\\right.$     \n",
    "  $\\lambda>0$   \n",
    "  \n",
    "  **Note:** the only difference between above density and Laplacians density is that Laplace has $|x|$ which makes it symmetric to $x$, and as a result, it also needs $\\frac{1}{2}$ coefficient. \n",
    "  \n",
    "  Let;s compute CDF of $X$:\n",
    "   $$F_X(x) = \\int_{-\\infty}^x f_X(y)dy \\\\ f_X\\text{ is }0 \\text{ if  }y < 0 \\\\ \\int_0^x f_X(y) dy = \\int_0^x \\lambda e^{-\\lambda y} dy = \\left. -e^{-\\lambda}\\right]_0^x \\\\ = -e^{-\\lambda x} + e^0 = 1 - e^{-\\lambda x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have proved that $F_X(x) = 1 - e^{-\\lambda x}$ for $x\\ge 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Waiting time interpretation\n",
    "\n",
    "Given that we have already waited $a$ times for an event to happen. What is the probability that we should wait extra $h$ longer time for the event to happen?\n",
    "\n",
    "$$\\mathbf{P}[X > a + h | X > a] = \\frac{\\mathbf{P}[X > a+h \\ and \\ X>a]}{\\mathbf{P}[X>a]} \\\\ = \\frac{\\mathbf{P}[X > a+h]}{\\mathbf{P}[X>a]} \\\\ =\\displaystyle  \\frac{e^{-\\lambda (a+h)}}{e^{-\\lambda (a)}} = e^{-\\lambda (h)}$$\n",
    "\n",
    "The results does not depend on \"a\". This means the exponential function has the ***\"memoryless\"*** property."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison with Geometric Distribution\n",
    "\n",
    "Recall: Geometric distribution was for a random variable describing how long we should wait to get the first success.\n",
    "\n",
    "Let $Y\\sim Geom(p)$, then the \"survival property\" $\\mathbf{P}[Y > k]$ where $k$ is integer. $\\displaystyle \\mathbf{P}[Y>k]= \\mathbf{P}[0000000\\text{  (k times)}] = (1-p)^k=\\displaystyle  e^{\\displaystyle -k\\ln \\displaystyle (\\frac{1}{1-p})}$\n",
    "\n",
    "Compare with $\\mathbf{P}[X > a] = e^{-\\lambda a}$.\n",
    "\n",
    "We see that this is actually the same formula: the value $\\displaystyle \\ln(\\frac{1}{1-p})$ plays the role of $\\lambda$ and therefore, $Y$ also has the ***memoryless*** property.\n",
    "\n",
    "**Remark:** Essentially, if a continous random variable $X$ has the memoryless property, then $X$ is exponential. Same thing with $Y$ discrete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other properties\n",
    "\n",
    " * Symmetry: when $X$ and $-X$ have the same CDF (*same CDF, not symmetric CDF*)\n",
    "   * Example: let $Y$ and $Y'$ be $\\sim Expn(\\lambda)$ and $Y$ and $Y'$ are independent. Then, define $X=Y-Y'$ and $\\tilde(X) = Y' - Y$. Clearly, $X$ and $\\tilde{X}$ have the same distribution, and $X$ is Symmetric.\n",
    "   Note: When $X$ is symmetric, with a density $f_X(x) = f_X(-x) \\text{ \"f_X is even\"}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantiles\n",
    "\n",
    "**Definition:**\n",
    "\n",
    "Let $\\alpha \\in [0,1]$. Let $X$ be a random variable. Then, there exists a value $x_\\alpha$ such that $mathbf{P}[[X \\le x_\\alpha] = \\alpha$. We say $x_\\alpha$ is the $\\alpha$th quantile of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Transformation\n",
    "\n",
    " Let $X$ have density $f_X$. Let $a,b$ be constants. Let $Y=aX+b$, then $Y$ has a density like this: $\\displaystyle f_Y(y) = f_X(\\displaystyle  \\frac{x-b}{a})\\times \\frac{1}{a}$\n",
    " \n",
    " \n",
    "It can be proven as follows: $F_Y(y) = \\mathbf{P}[aX+b \\le y] = \\mathbf{P}[\\displaystyle X\\le \\frac{y-b}{a}] \\Rightarrow \\ \\text{ by definition } = F_X(\\displaystyle \\frac{y-b}{a})$\n",
    "\n",
    "Therefore, $f_Y(y) = \\frac{dF_Y}{dy} = \\frac{1}{a} F'_X(\\frac{y-b}{a}) = \\frac{1}{a} f_X(\\frac{y-b}{a})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expectations\n",
    "\n",
    "Let $X$ has density $f_X$. Then, $\\displaystyle \\mathbf{E}[X] = \\displaystyle \\int_{-\\infty}^{+\\infty} x f_X(x) dx$ \n",
    "\n",
    "Same idea as discrete case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example:\n",
    "\n",
    "$X \\sim Unif[0,1]$. Then $\\mathbf{E}[X] = \\int_0^1 1\\times x dx = \\frac{1}{2}x^2 = \\frac{1}{2}\\times(1 - 0) = \\frac{1}{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Property:** Linearity works.\n",
    "\n",
    "##### Example:\n",
    "\n",
    "Let $Y\\sim Unif[a,b]$. We claim that $Y=(b-a)X + a$ where $X\\sim Unif[0,1]$.\n",
    "\n",
    "Therefore, $\\mathbf{E}[Y] = (b-a)\\times \\mathbf{E}[X] + a = \\frac{a+b}{2}$\n",
    "\n",
    "We have used the linearity of expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition:\n",
    "\n",
    "Let $X$ have density $f_X$, and let $g$ be a function. Then $\\mathbf{E}[g(X)] =\\displaystyle  \\int_{-\\infty}^{\\infty} g(x) f_X(x) dx$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition: variance\n",
    "\n",
    "$$\\mathbf{Var}[X] = \\mathbf{E}[\\displaystyle  (X-\\mathbf{E}[X])^2]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Property**\n",
    " \n",
    "$$\\mathbf{Var}[aX + v] = a^2 \\mathbf{Var}[X]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example:\n",
    "\n",
    "Let $Y\\sim Uni[0,1]$. What is variance of $Y$?\n",
    "\n",
    "Recall thay $\\mathbf{E}[Y] = \\frac{a+b}{2}$ and $\\mathbf{Var}[Y] = \\mathbf{Var}[(b-a)X + a] = (b-a)^2 \\mathbf{Var}[X]$.\n",
    "\n",
    "Now, $$\\mathbf{Var}[X] = \\mathbf{E}[X^2] - (\\mathbf{E}[X])^2 $$\n",
    "\n",
    "$\\mathbf{E}[X^2] = \\int_0^1 x^2 dx = \\frac{1}{3}$\n",
    "\n",
    "So, $\\mathbf{Var}[X] = \\frac{1}{3} - (\\frac{1}{2})^2 = \\frac{1}{12}$\n",
    "\n",
    "Finally, $\\mathbf{Var}[Y] = (b-a)^2 \\mathbf{Var}[X] = \\frac{(b-a)^2}{12}$\n",
    "\n",
    "\n",
    "Compare this with the formula in the discrete case $\\{1,2,...N\\}$, $\\mathbf{Var}[] = \\frac{N(N-1)}{12}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review: integration by parts\n",
    "\n",
    "$$\\int_a^b u\\ dv = \\left.uv\\right]_a^b - \\int_a^b v du $$\n",
    "\n",
    "where $du$ is the differential of $u$ and $dv$ is the differential of v.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example:\n",
    "\n",
    "Let $X\\sim Expn(\\lambda)$. What is $\\mathbf{E}[X]$?\n",
    "\n",
    "$\\mathbf{E}[X] = \\int_0^{+\\infty}\\lambda e^{-\\lambda x} \\ x\\ dx $\n",
    "\n",
    "Solve the integral by \"integration by parts\": \n",
    "  * $u = x \\Rightarrow du = dx$ and \n",
    "  * $dv = \\lambda e^{-\\lambda x} dx \\Rightarrow v = - e^{-\\lambda x} dx$\n",
    "\n",
    "\n",
    "$$\\int_0^{\\infty} x \\lambda e^{-\\lambda x} dx = \\left.-x e^{-\\lambda x} dx\\right]_0^\\infty - \\int_0^\\infty -e^{-\\lambda x} dx = \\\\ \\left.\\frac{1}{\\lambda} e^{-\\lambda x}\\right]_0^\\infty = \\frac{1}{\\lambda}$$\n",
    "\n",
    "We proved that for $X\\sim Expon(\\lambda)$, then $\\mathbf{E}[X] =\\displaystyle  \\frac{1}{\\lambda}$   \n",
    "\n",
    "This matches the idea that since the average number of arrivals of a Poisson process with parameter $\\lambda$ in an interval of time 1, is $=\\lambda$, it might be totally true and very satisfying that the average wait for any event is $=\\frac{1}{\\lambda}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moments of $Expon(\\lambda)$\n",
    "\n",
    "$$X\\sim Expon(\\lambda) \\ \\ \\ \\ \\Longrightarrow \\ \\ \\ \\ \\ \\mathbf{E}[X^p] = \\frac{p!}{\\lambda ^p}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the moments relation for exponential random variable, we can easily find the variance:\n",
    "\n",
    "  * $p=2:   \\ \\ \\ \\mathbf{E}[X^2] = \\frac{2}{\\lambda^2}$\n",
    "  * $\\mathbf{Var}[X] = \\mathbf{E}[X^2] - (\\mathbf{E}[X])^2 = \\displaystyle \\frac{2}{\\lambda^2} - \\frac{1}{\\lambda^2} = \\frac{1}{\\lambda^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformation of random variables\n",
    "\n",
    "Let $X\\sim Expon(1)$.  Let $\\lambda>0$ be fixed. Let $Y=\\frac{1}{\\lambda} X$. Let's find density of $Y$.\n",
    "\n",
    "By our linear transformation for density: $$f_Y(y) =\\displaystyle  \\frac{1}{1/\\lambda} f_X \\left(\\frac{y - 0}{1/\\lambda}\\right) = \\lambda e^{-\\lambda y}$$\n",
    "\n",
    "Here, we recognize thet $Y \\sim Expon(\\lambda)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Jelly-Donut Problem: a mixed distribution\n",
    "\n",
    "Some distributions are mixed, with a density and a discrete part. For example, we can define\n",
    "\n",
    "$$X = \\left\\{\\begin{array}{lcr} Y & if & Y\\le B\\\\B & & otherwise\\end{array}\\right.$$ \n",
    "\n",
    "where $Y$ is a random variable with density and $B$ is not random. There $X$ has a density below $B$ and $B$ is an ... for $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0rc4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
