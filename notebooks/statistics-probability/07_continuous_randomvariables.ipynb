{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continous Random variables\n",
    "==========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Recall the PMF for a **discrete** random variable $X$ for any $x\\in \\mathbb{Z}$: $p_X(x) = \\mathbf{P}[X=x]$   \n",
    " Then, we can write $$\\mathbf{P}[n \\le X \\le m] = \\sum_{i=n}^m p_X(i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition:\n",
    "\n",
    "We say that a random variable $X$ is continous and has density $f_X$ if the probability $\\displaystyle \\mathbf{P}[a \\le X \\le b] = \\int_a^b f_X(x) dx$\n",
    "\n",
    "\n",
    "(This is because of Riemann's sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, ***symbolically***: $\\displaystyle \\mathbf{P}[x\\le X\\le x+dx] = f_X(x) dx$\n",
    "\n",
    "This shows that $f_X(x)$ by itself is not a probability, but rather some sort of *probability intensity*. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition:**\n",
    "\n",
    "If $X$ is a random variable, (discrete, continous, or otherwise), we let $F_X(x) = \\mathbf{P}[X\\le x]$ for any $x\\in \\mathbb{R}$.   \n",
    "This $F_X(x)$ is called the **cumulative distribution function** of $X$. \n",
    "\n",
    "Note: if $X$ has density $f_X$, then $\\displaystyle F_X(x) = \\int_{-\\infty}^x f_X(x) dx$\n",
    "\n",
    "This is from the definition od $f_X$ with $a=-\\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In this case, we also have $\\displaystyle \\frac{dF_X}{dx} f_X(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This follows directly from the so-called \"fundamental theorem of Calculus\".\n",
    "\n",
    "Some facts: \n",
    " * Also, when $X$ has a density, then $F_X$ is continous.\n",
    "\n",
    " * $\\displaystyle \\int_{-\\infty}^\\infty f(x) dx = 1$     \n",
    " \n",
    " * $f_X(x) \\ge 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Properties:**\n",
    "\n",
    "\n",
    " \n",
    " * $F_X$ increases from $0$ to $1$   \n",
    " \n",
    " * The limits $0$ and $1$ at $-\\infty$ and $+\\infty$ exists because of \"monotonous convergence of probabilities\".   \n",
    " \n",
    " * For $X$ with density $f_X$, the value of $f_X$ at a single point or even a countable collection of points does not matter.   \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Examples\n",
    "\n",
    "  * Example 1: (Fundamental): Uniform density on $[0,1]$. $f_X(x) = \\left\\{\\begin{array}{lr}1 & \\text{if }x\\in[0,1] \\\\ 0 & \\text{otherwise}\\end{array}\\right.$    \n",
    "  \n",
    "  * Example 2: $X$ with density $f_X(x) = \\left\\{\\begin{array}{lr}constant & \\text{if }x\\in [0,1] \\\\ 0 & \\text{otherwise}\\end{array}\\right.$      \n",
    "    We conclude that $\\int_{-\\infty}^\\infty f_X(x) dx = 1 \\ \\ \\Longrightarrow constant = 2$\n",
    "    \n",
    "  * Example 3:  $X$ with density $f_X(x) = \\left\\{\\begin{array}{lr}constant & \\text{if }x\\in [a,b] \\\\ 0 & \\text{otherwise}\\end{array}\\right.$     \n",
    "     Then the constant is $\\displaystyle constant = \\frac{1}{b-a}$   \n",
    "\n",
    "  * Example 4: $X$ with density $\\displaystyle f_X(x) = \\left\\{\\begin{array}{lr}\\displaystyle \\frac{1}{2}x & \\text{if }x\\in [0,2] \\\\ 0 & \\text{otherwise}\\end{array}\\right.$   \n",
    "  \n",
    "  * Example 5: (fundamental) $X$ with density $\\displaystyle f_X(x) = \\left\\{\\begin{array}{lr}\\displaystyle \\lambda e^{-\\lambda x} & \\text{for }x > 0 \\\\ 0 & \\text{otherwise}\\end{array}\\right.$     \n",
    "  $\\lambda>0$   \n",
    "  \n",
    "  **Note:** the only difference between above density and Laplacians density is that Laplace has $|x|$ which makes it symmetric to $x$, and as a result, it also needs $\\frac{1}{2}$ coefficient. \n",
    "  \n",
    "  Let;s compute CDF of $X$:\n",
    "   $$F_X(x) = \\int_{-\\infty}^x f_X(y)dy \\\\ f_X\\text{ is }0 \\text{ if  }y < 0 \\\\ \\int_0^x f_X(y) dy = \\int_0^x \\lambda e^{-\\lambda y} dy = \\left. -e^{-\\lambda}\\right]_0^x \\\\ = -e^{-\\lambda x} + e^0 = 1 - e^{-\\lambda x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have proved that $F_X(x) = 1 - e^{-\\lambda x}$ for $x\\ge 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Waiting time interpretation\n",
    "\n",
    "Given that we have already waited $a$ times for an event to happen. What is the probability that we should wait extra $h$ longer time for the event to happen?\n",
    "\n",
    "$$\\mathbf{P}[X > a + h | X > a] = \\frac{\\mathbf{P}[X > a+h \\ and \\ X>a]}{\\mathbf{P}[X>a]} \\\\ = \\frac{\\mathbf{P}[X > a+h]}{\\mathbf{P}[X>a]} \\\\ =\\displaystyle  \\frac{e^{-\\lambda (a+h)}}{e^{-\\lambda (a)}} = e^{-\\lambda (h)}$$\n",
    "\n",
    "The results does not depend on \"a\". This means the exponential function has the ***\"memoryless\"*** property."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison with Geometric Distribution\n",
    "\n",
    "Recall: Geometric distribution was for a random variable describing how long we should wait to get the first success.\n",
    "\n",
    "Let $Y\\sim Geom(p)$, then the \"survival property\" $\\mathbf{P}[Y > k]$ where $k$ is integer. $\\displaystyle \\mathbf{P}[Y>k]= \\mathbf{P}[0000000\\text{  (k times)}] = (1-p)^k=\\displaystyle  e^{\\displaystyle -k\\ln \\displaystyle (\\frac{1}{1-p})}$\n",
    "\n",
    "Compare with $\\mathbf{P}[X > a] = e^{-\\lambda a}$.\n",
    "\n",
    "We see that this is actually the same formula: the value $\\displaystyle \\ln(\\frac{1}{1-p})$ plays the role of $\\lambda$ and therefore, $Y$ also has the ***memoryless*** property.\n",
    "\n",
    "**Remark:** Essentially, if a continous random variable $X$ has the memoryless property, then $X$ is exponential. Same thing with $Y$ discrete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other properties\n",
    "\n",
    " * Symmetry: when $X$ and $-X$ have the same CDF (*same CDF, not symmetric CDF*)\n",
    "   * Example: let $Y$ and $Y'$ be $\\sim Expn(\\lambda)$ and $Y$ and $Y'$ are independent. Then, define $X=Y-Y'$ and $\\tilde(X) = Y' - Y$. Clearly, $X$ and $\\tilde{X}$ have the same distribution, and $X$ is Symmetric.\n",
    "   Note: When $X$ is symmetric, with a density $f_X(x) = f_X(-x) \\text{ \"f_X is even\"}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantiles\n",
    "\n",
    "**Definition:**\n",
    "\n",
    "Let $\\alpha \\in [0,1]$. Let $X$ be a random variable. Then, there exists a value $x_\\alpha$ such that $mathbf{P}[[X \\le x_\\alpha] = \\alpha$. We say $x_\\alpha$ is the $\\alpha$th quantile of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Transformation\n",
    "\n",
    " Let $X$ have density $f_X$. Let $a,b$ be constants. Let $Y=aX+b$, then $Y$ has a density like this: $\\displaystyle f_Y(y) = f_X(\\displaystyle  \\frac{x-b}{a})\\times \\frac{1}{a}$\n",
    " \n",
    " \n",
    "It can be proven as follows: $F_Y(y) = \\mathbf{P}[aX+b \\le y] = \\mathbf{P}[\\displaystyle X\\le \\frac{y-b}{a}] \\Rightarrow \\ \\text{ by definition } = F_X(\\displaystyle \\frac{y-b}{a})$\n",
    "\n",
    "Therefore, $f_Y(y) = \\frac{dF_Y}{dy} = \\frac{1}{a} F'_X(\\frac{y-b}{a}) = \\frac{1}{a} f_X(\\frac{y-b}{a})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expectations\n",
    "\n",
    "Let $X$ has density $f_X$. Then, $\\displaystyle \\mathbf{E}[X] = \\displaystyle \\int_{-\\infty}^{+\\infty} x f_X(x) dx$ \n",
    "\n",
    "Same idea as discrete case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example:\n",
    "\n",
    "$X \\sim Unif[0,1]$. Then $\\mathbf{E}[X] = \\int_0^1 1\\times x dx = \\frac{1}{2}x^2 = \\frac{1}{2}\\times(1 - 0) = \\frac{1}{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Property:** Linearity works.\n",
    "\n",
    "##### Example:\n",
    "\n",
    "Let $Y\\sim Unif[a,b]$. We claim that $Y=(b-a)X + a$ where $X\\sim Unif[0,1]$.\n",
    "\n",
    "Therefore, $\\mathbf{E}[Y] = (b-a)\\times \\mathbf{E}[X] + a = \\frac{a+b}{2}$\n",
    "\n",
    "We have used the linearity of expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition:\n",
    "\n",
    "Let $X$ have density $f_X$, and let $g$ be a function. Then $\\mathbf{E}[g(X)] =\\displaystyle  \\int_{-\\infty}^{\\infty} g(x) f_X(x) dx$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition: variance\n",
    "\n",
    "$$\\mathbf{Var}[X] = \\mathbf{E}[\\displaystyle  (X-\\mathbf{E}[X])^2]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Property**\n",
    " \n",
    "$$\\mathbf{Var}[aX + v] = a^2 \\mathbf{Var}[X]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example:\n",
    "\n",
    "Let $Y\\sim Uni[0,1]$. What is variance of $Y$?\n",
    "\n",
    "Recall thay $\\mathbf{E}[Y] = \\frac{a+b}{2}$ and $\\mathbf{Var}[Y] = \\mathbf{Var}[(b-a)X + a] = (b-a)^2 \\mathbf{Var}[X]$.\n",
    "\n",
    "Now, $$\\mathbf{Var}[X] = \\mathbf{E}[X^2] - (\\mathbf{E}[X])^2 $$\n",
    "\n",
    "$\\mathbf{E}[X^2] = \\int_0^1 x^2 dx = \\frac{1}{3}$\n",
    "\n",
    "So, $\\mathbf{Var}[X] = \\frac{1}{3} - (\\frac{1}{2})^2 = \\frac{1}{12}$\n",
    "\n",
    "Finally, $\\mathbf{Var}[Y] = (b-a)^2 \\mathbf{Var}[X] = \\frac{(b-a)^2}{12}$\n",
    "\n",
    "\n",
    "Compare this with the formula in the discrete case $\\{1,2,...N\\}$, $\\mathbf{Var}[] = \\frac{N(N-1)}{12}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review: integration by parts\n",
    "\n",
    "$$\\int_a^b u\\ dv = \\left.uv\\right]_a^b - \\int_a^b v du $$\n",
    "\n",
    "where $du$ is the differential of $u$ and $dv$ is the differential of v.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example:\n",
    "\n",
    "Let $X\\sim Expn(\\lambda)$. What is $\\mathbf{E}[X]$?\n",
    "\n",
    "$\\mathbf{E}[X] = \\int_0^{+\\infty}\\lambda e^{-\\lambda x} \\ x\\ dx $\n",
    "\n",
    "Solve the integral by \"integration by parts\": \n",
    "  * $u = x \\Rightarrow du = dx$ and \n",
    "  * $dv = \\lambda e^{-\\lambda x} dx \\Rightarrow v = - e^{-\\lambda x} dx$\n",
    "\n",
    "\n",
    "$$\\int_0^{\\infty} x \\lambda e^{-\\lambda x} dx = \\left.-x e^{-\\lambda x} dx\\right]_0^\\infty - \\int_0^\\infty -e^{-\\lambda x} dx = \\\\ \\left.\\frac{1}{\\lambda} e^{-\\lambda x}\\right]_0^\\infty = \\frac{1}{\\lambda}$$\n",
    "\n",
    "We proved that for $X\\sim Expon(\\lambda)$, then $\\mathbf{E}[X] =\\displaystyle  \\frac{1}{\\lambda}$   \n",
    "\n",
    "This matches the idea that since the average number of arrivals of a Poisson process with parameter $\\lambda$ in an interval of time 1, is $=\\lambda$, it might be totally true and very satisfying that the average wait for any event is $=\\frac{1}{\\lambda}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moments of $Expon(\\lambda)$\n",
    "\n",
    "$$X\\sim Expon(\\lambda) \\ \\ \\ \\ \\Longrightarrow \\ \\ \\ \\ \\ \\mathbf{E}[X^p] = \\frac{p!}{\\lambda ^p}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the moments relation for exponential random variable, we can easily find the variance:\n",
    "\n",
    "  * $p=2:   \\ \\ \\ \\mathbf{E}[X^2] = \\frac{2}{\\lambda^2}$\n",
    "  * $\\mathbf{Var}[X] = \\mathbf{E}[X^2] - (\\mathbf{E}[X])^2 = \\displaystyle \\frac{2}{\\lambda^2} - \\frac{1}{\\lambda^2} = \\frac{1}{\\lambda^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship between Poisson and Exponential\n",
    "\n",
    "\n",
    "Let $X\\sim Exp(\\lambda)$, then survival fimction $\\mathbf{P}[X\\ge x] = \\displaystyle e^{\\displaystyle  -\\lambda x}$.\n",
    "\n",
    "$X$ is interpreted as a waiting time. $X$ is ***memory-less***; the key-property of exponential random variables.\n",
    "\n",
    "\n",
    "The rest of the story is that the inter-arrival times of a Poisson process are independent of each other, and are exponentially distributed.\n",
    "\n",
    "In fact, there is an equivalence between saying that events arrive according to such i.i.d. exponentially distributed inter-arrival times and saying that the arrival times are the jump times of a Poisson process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Jelly-Donut Problem: a mixed distribution\n",
    "\n",
    "Some distributions are mixed, with a density and a discrete part. For example, we can define\n",
    "\n",
    "$$X = \\left\\{\\begin{array}{lcr} Y & if & Y\\le B\\\\B & & otherwise\\end{array}\\right.$$ \n",
    "\n",
    "where $Y$ is a random variable with density and $B$ is not random. There $X$ has a density below $B$ and $B$ is an ... for $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Survival function\n",
    "\n",
    "Let $X$ be a random variable with density $f_X(x)$ and CDF $F_X(x), \n",
    "then surviaval function of $\\mathbf{P}[X > x] = 1 - F_X(x)$\n",
    "\n",
    "For exponential random variable $X\\sim Exp(\\lambda) \\Longrightarrow \\mathbf{P}[X >x] = 1-F_X(x) = 1 - (1-e^{-\\lambda x}) = e^{-\\lambda x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation of random variables\n",
    "\n",
    "Let $X\\sim Expon(1)$.  Let $\\lambda>0$ be fixed. Let $Y=\\frac{1}{\\lambda} X$. Let's find density of $Y$.\n",
    "\n",
    "By our linear transformation for density: $$f_Y(y) =\\displaystyle  \\frac{1}{1/\\lambda} f_X \\left(\\frac{y - 0}{1/\\lambda}\\right) = \\lambda e^{-\\lambda y}$$\n",
    "\n",
    "Here, we recognize thet $Y \\sim Expon(\\lambda)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example:\n",
    "\n",
    "Let $U\\sim Unif(0,1)$, then $\\displaystyle f_U(u) = \\left\\{\\begin{array}{lrr} 1 & if & u\\in [0,1]\\\\0 & & otherwise\\end{array}\\right.$\n",
    "\n",
    "Ley $Y=-\\ln U$. We want to find density of $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "First of all, $Y > 0$.\n",
    "\n",
    "Let's try to compute the CDF of $Y$:   $$F_Y(y) = \\mathbf{P}[Y\\le y] = \\left\\{\\begin{array}{lcr} 0 & if & y \\le 0 \\\\ \\mathbf{P}[\\ln U \\ge -y] && otherwise\\end{array}\\right.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathbf{P}[\\ln U\\ge -y] = \\mathbf{P}[U \\ge e^{\\displaystyle -y}]$$\n",
    "\n",
    "Note that in order for above to be legitimate, $e^{-y}$ has to be between [0,1]. And in fact it is since $y>0$.\n",
    "\n",
    "Next, since $U\\sim Unif(0,1)$, then for any $ 0 \\le a \\le 1$: $\\mathbf{P}[U \\ge a] = 1 - \\mathbf{P}[U \\le a] = 1 - a$\n",
    "\n",
    "Therefore, $$\\mathbf{P}[U \\ge e^{-y}] = 1 - \\mathbf{P}[U \\le e^{-y}] = 1 - e^{-y}$$ and we recognize that this is infact the CDF of exponential random variable. So, $Y \\sim Exp(\\lambda = 1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note: simulating random variables $\\sim Exp(1)$\n",
    "\n",
    "We have proved that to simulate a drawe of a random variable which is $Exp(\\lambda=1)$, we just nneed to draw a random variable in $(0,1)$ and take the opposite of its $\\ln$.\n",
    "\n",
    "If you want to simulate an exponential random variable, we can generate uniform random numbers, and transform them according to above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simulating random variables $\\sim Exp(\\lambda)$\n",
    "\n",
    "If $X\\sim Exp(\\lambda)$ then survival function $\\mathbf{P}[X > x] = e^{\\displaystyle -\\lambda x}$\n",
    "\n",
    "$$\\mathbf{P}[Y < \\lambda x] = e^{-\\lambda x}$$\n",
    "\n",
    "$$\\mathbf{P}[\\frac{1}{\\lambda} Y > x] = e^{-\\lambda x}$$\n",
    "\n",
    "Let's call $Z=\\frac{1}{\\lambda}Y$. Then $\\mathbf{P}[Z > x] = e^{-\\lambda x}$ therefore, we recognize that $Z\\sim Exp(\\lambda)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, this shows that $\\frac{1}{\\lambda}$ is a so-called scale paramater for the exponential distribution. In particular, by multiplying an exponential random variable by a number, we get another exponential random variable.\n",
    "\n",
    "Moreover, generally for any class of random variables with $CDF=F(x;c)$ we say that $c$ is a scale parameter of $F(x;c) = \\tilde{F}(\\frac{x}{c})$\n",
    "\n",
    "Note: for densities this translates as $f(x;c) = \\frac{1}{c} ~ \\tilde{f}(\\frac{x}{c})$\n",
    "\n",
    "##### Example:\n",
    "\n",
    "$Exp(\\lambda)$ has $\\frac{1}{\\lambda}$ as a scale parameter because \n",
    "\n",
    "$$F_X(x) = 1 - e^{\\displaystyle -\\lambda x} = 1 - e^{\\displaystyle -\\frac{x}{\\frac{1}{\\lambda}}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theorem\n",
    "\n",
    "Let $X$ have density $f_X$\n",
    "\n",
    "Let $h$ be a strictly monotone function(either always decreasing or always increasing)\n",
    "\n",
    "Let $Y=h(X)$ so $Y$ is also a random variable and has a density $f_Y$\n",
    "\n",
    "$$f_Y(y) = f_X(h^{-1}(y)) \\frac{1}{\\frac{d~ h }{dx} \\left(h^{-1}(y)\\right)}$$\n",
    "\n",
    "##### Proof:\n",
    "\n",
    "Recall that $Y$ has a density $f_Y$ f we can write symbolically $\\mathbf{P}[Y \\in dy] = f_Y(y) dy$\n",
    "\n",
    "$$\\mathbf{P}[Y \\in dy] = \\mathbf{P}[h(x) \\in dy] = \\mathbf{P}[X \\in h^{-1}(dy)] \\\\ = \\mathbf{P}[X \\in~ (\\text{interval around the value }h^{-1}(y)\\text{ of width }h^{-1}(dy) )] \\\\ =f_X\\left(h^{-1}(y)\\right) \\times \\text{width of that interval} $$\n",
    "\n",
    "$h^{-1}(y) = \\frac{d~x}{dy} dy = \\displaystyle \\frac{d ~h^{-1}(Y)}{dy} dy = \\displaystyle \\frac{1}{\\displaystyle \\frac{d~y}{dx}} dy$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of application of the general therom\n",
    "\n",
    "Let $U\\sim Unif(0,1)$. Let $F$ be the CDF of a random variable. Assume that $F$ is stritly inxreasing. \n",
    "\n",
    "Therefore, $F^{-1}(u)$ is defined for every $u\\in(0,1)$. Moreover, $X = F^{-1}(U)$ has $F$ as its CDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joind Densities\n",
    "\n",
    "A pair of random variables $(X,Y)$ has a density $f_{X,Y}$ if for any intervals $[a,b]$ and $[c,d]$ the probability $$\\mathbf{P}\\left[X\\in [a,b] \\& Y \\in [c,d]\\right] = \\int_a^b \\left( \\displaystyle \\int_c^d f_{X,Y} (x,y)~dy \\right)dx$$\n",
    "\n",
    "or sometimes written as $${\\int \\displaystyle \\int}_{[a,b]\\times[c,d]} f_{X,Y}(x,y) ~dx ~dy$$\n",
    "\n",
    "Note: $[a,b]\\times [c,d]$ is a rectangle that looks like this \n",
    "\n",
    "<img src=\"figs/rect.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition\n",
    "\n",
    "Let $g$ be a function on $\\mathbb{R}\\times \\mathbb{R}$. Then, $$\\mathbf{E}[g(X,Y)] = \\displaystyle \\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty g(x,y) f_{X,Y}(x,y) ~dy~dx$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independence\n",
    "\n",
    "If $X$ and $Y$ are independent, then $\\mathbf{E}[XY] = \\mathbf{E}[X] \\mathbf{E}[Y]$\n",
    "\n",
    "Equally importantly, if $f_X$ and $f_Y$ are densities for $X$ and $Y$, then the pair $(X,Y)$ has density $$f_{X,Y}(x,y) = f_X(x) f_Y(y)$$\n",
    "\n",
    "\n",
    "##### Proof:\n",
    "\n",
    "$$\\mathbf{P}[(X,Y)\\in dx~dy] = \\mathbf{P}[X\\in dx \\ \\&\\  Y\\in dy] \\\\ = \\mathbf{P}[X\\in dx] \\mathbf{P}[Y\\in dy]= f_X(x)~dx\\ \\times\\ f_Y(y)~dy = f_X(x)f_Y(y)~dx~dy $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example\n",
    "\n",
    "Let $(X,Y)$ have density $f_{x,Y}(x,y) = \\left\\{\\begin{array}{lrr}15 e^{-5x-3y} & if & x<0 and y>0\\\\ 0 && otherwise\\end{array}\\right.$\n",
    "\n",
    "What is the distribution of $(X,Y)$?\n",
    "\n",
    "$$15 e^{-5x-3y}  = 5e^{-5x} \\times 3e^{-3y} \\\\ 5e^{-5x}\\sim Exp(5) \\ \\ \\ \\ 3e^{-3y}\\sim Exp(3)$$\n",
    "\n",
    "Since $X$ and $Y$  have zero density on negative values, this is really the product of these densities. Therefore, $x\\&Y$ are indepedent, and $X\\simExp(5)$ and $Y\\sim Exp(3)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example\n",
    "\n",
    "Let $(X,Y)$ have density $f_{X,Y} = \\left\\{\\begin{array}{lrr} 8xy & for & 0\\le x \\le y \\le 1 \\\\ 0 &&otherwise\\end{array}\\right.$\n",
    "\n",
    "Note: from the condition $0\\le x \\le y \\le 1 \\\\ 0$ we can get that $X \\le Y$. So there is a relation between them, therefore we are pretty sure that $X\\&Y$ are **NOT** independent because of the order between them.\n",
    "\n",
    "\n",
    "<img src=\"figs/triangle-X-Y.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, we should check that $\\displaystyle \\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty f_{X,Y}(x,y) ~dx ~dy = 1 $\n",
    "\n",
    "$\\displaystyle \\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty f_{X,Y}(x,y) ~dx ~dy = \\displaystyle \\int_0^1 \\left(\\int_0^y f_{X,Y}(x,y) ~dx \\right)~dy = 1  $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______\n",
    "\n",
    "##### Example\n",
    "\n",
    "Witht the same $X\\&Y$ as given above, let $T=X+Y$ find probability $\\mathbf{P}[T>1]$?\n",
    "\n",
    "since we want $T>1$, and we know that $X<Y$, so the minimum possible value for $Y$ is $0.5$,\n",
    "\n",
    "\n",
    "$$\\mathbf{P}[X+Y > 1] = \\int_\\frac{1}{2}^1 \\int_{\\text{over all possible values of }x\\text{ when }y\\text{ is fixed}} 8xy ~dx ~dy \\\\= \\mathbf{P}[X > 1 - Y] = \\int_\\frac{1}{2}^1 \\int_{1-y}^1 8xy ~dx ~dy$$\n",
    "\n",
    "when $1-y < y$? This occurs if $y>\\frac{1}{2}$ and it is already satisfied from the outer interal.\n",
    "\n",
    "So, therefore, $$\\mathbf{P}[T>1] = \\int_\\frac{1}{2}^1 \\int_{1-y}^1 8xy ~dx ~dy \\\\ =  \\int_\\frac{1}{2}^1 8y \\int_{1-y}^1 x ~dx ~dy = \\int_\\frac{1}{2}^1 8y\\left(\\frac{1}{2}x^2\\right)_{1-y}^y dy $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\\\ = \\int_\\frac{1}{2}^1 \\frac{1}{2} 8y\\left(y^2 - (1-y)^2)_{1-y}^y dy = \\int_\\frac{1}{2}^1 4y\\left(y^2 - y^2 +2y - 1) dy $$\n",
    "\n",
    "\n",
    "$$\\\\ = \\int_\\frac{1}{2}^1 8y^2 - 4y dy = \\left(\\frac{8}{3}y^3 - 2y^2\\right)_\\frac{1}{2}^1 = ..$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition\n",
    "\n",
    "If $(X,Y)$ has a density $f_{X,Y}$, then, we say that the densities of $X\\&Y$ separately are the arginals of $X,Y$\n",
    "\n",
    "#### Theorem\n",
    "\n",
    "In this case, $f_X(x) = \\displaystyle \\int_{-\\infty}^\\infty f_{X,Y}(x,y) dy$   \n",
    "Similarly for $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Example:\n",
    "\n",
    "Let $f_{X,Y} = \\left\\{\\begin{array}{lrr}\\displaystyle 2e^{\\displaystyle -x-y}&for& 0\\le y <x <\\infty \\\\ 0 &&otherwise\\end{array}\\right.$\n",
    "\n",
    "Let's find $f_X(x)=?$\n",
    "\n",
    "$$f_X(x) = \\int_{-\\infty}^\\infty f_{X,Y}(x,y) dy = \\int_0^x 2e^{-x-y} dy = 2e^{-x} \\int_0^x e^{-y}dy = 2e^{-x} \\left(-e^{-y}\\right)_0^x = 2e^{-x}\\left(1 - e^{-x}\\right) \\ \\ \\text{for }x\\ge 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find $f_Y(y)=?$\n",
    "\n",
    "$$f_Y(y) = 2\\int_y^\\infty 2e^{-x-y} dx = 2e^{-y} \\int_y^\\infty e^{-x}dx = 2e^{-y} \\left(e^{-x}\\right)_y^\\infty = 2e^{-y}\\left(0 + e^{-y}\\right) = 2e^{-2y} \\sim Exp(2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this example, let $T=X+Y$. Does $T$ have a density?\n",
    "\n",
    "Let's find the CDF $F_T$ and look for $\\frac{d~F_T}{dt}$. We compute for $t>0$. \n",
    "\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0rc4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
