{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Theorem (Sufficient Condition for Convergence in Distribution)\n",
    "\n",
    "(Also known as  **the continuity theorem**)\n",
    "\n",
    "Let $X_n$ be a sequence of random variables, and $M_{X_n}(t) = \\mathbf{E}\\left[e^{tX_n}\\right]$ is the corresponding sequence of moment generating functions.\n",
    "\n",
    "If $_{X_n}(t) \\to M_X(t) \\text{ as } n\\to \\infty$ for $t$ closeto $0$, and $M_X$ is the MGF of $X$. Then, $X_n \\to X$ in distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: convergence in distribution is also called convergence in law."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theorem: Central Limit Theorem (CLT)\n",
    "\n",
    "Let $X_i: ~ i=1,..,n$ be $iid$ random variables.\n",
    "\n",
    "Assume $\\mu = \\mathbf{E}[X_i]$ and $\\sigma^2 = \\mathbf{Var}[X_i] < \\infty$.\n",
    "\n",
    "Let $Z_i = \\displaystyle \\frac{X_i - \\mu}{\\sigma}$ (Z_i is standardized).\n",
    "\n",
    "Let $S_n = \\displaystyle \\frac{\\sum_{i=1}^n Z_i}{\\sqrt{n}}$\n",
    "\n",
    "Then, $S_n \\to \\mathcal{N}(0,1) \\text{ as }n\\to \\infty$ (convergence in distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proof:\n",
    "\n",
    "Recall Taylor's formula: Let $M$ be a function defined for $t$ near $0$. Assume $M$ has $m$ continuous derivatives. Then $M(t) = \\displaystyle \\sum_{k=0}^m \\displaystyle M^{(k)} \\frac{1}{k!} t^k ~ + ~ \\epsilon(t) t^{m})$ where $M^{(k)}$ is th kth derivative of $M$ at $t=0$ and $\\epsilon(u) \\to 0 \\text{ as } u\\to 0$.\n",
    "\n",
    "\n",
    "Thsi theorem is a consequence of the mean-value theorem that says in the case $m=1$, for $b>0$ $M(b) - M(0) = (b-0)t M'(\\xi)$ where $\\xi \\in (0,b)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now, apply this Taylor's formula with $m=2$ to prove CLT:\n",
    "\n",
    "$M_{Z_i}(t) = \\sum_{k=0}^2 M^{(k)}_{Z_i} (0) \\frac{1}{k!} t^k ~+~ \\epsilon(t) t^2$\n",
    "\n",
    "Now, $$M_{Z_i}(t) = M_{Z_i}(0) ~+~ M'_{Z_i}(0) t + \\frac{1}{2} M''_{Z_i}(0) t^2$$\n",
    "\n",
    " * $M_{Z_i} (0) = 1$\n",
    "\n",
    " * $M'_{Z_i} (0) = \\text{first moment} = \\mathbf{E}[Z_i] = 0$   \n",
    "\n",
    " * $M'_{Z_i} (0) = \\text{second moment} = \\mathbf{E}[Z_i^2] =  \\mathbf{Var}[Z_i] - 0^2= 1$   \n",
    "\n",
    "therefore, $M_{Z_i}(t) = 1 + \\frac{1}{2} t^2 + \\epsilon(t^2) ~+~ \\epsilon(t) t^{2}$\n",
    "\n",
    "Therefore, for $S_n$ we have $\\displaystyle M_{Z_1 + Z_2 + ... +Z_n} = \\prod_{k=1}^n M_{Z_i}(t) = \\left(1 + \\frac{1}{2}t^2 + \\epsilon(t) t^{2}\\right)^n$\n",
    "\n",
    "And for $S_n = \\frac{}{\\sqrt{n}}$: \n",
    "\n",
    "$$\\displaystyle M_{S_n} (t) = M_{Z_1 + ... + Z_n}\\left(\\frac{t}{\\sqrt{n}} \\right) =\\\\ \\left(1 + \\frac{1}{2}\\frac{t^2}{n} + \\epsilon(\\frac{t}{\\sqrt{n}}) \\frac{t^2}{n}\\right) = \\\\ \\left(1 + \\frac{\\frac{1}{2} t^2 + \\epsilon(\\frac{t}{\\sqrt{n}}) t^2}{n}\\right)^n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just need to prove that this expression converges to $e^{t^2/2}$ because that is the MGF of $\\mathcal{N}(0,1)$.\n",
    "\n",
    "Just use the following lemma: \n",
    "\n",
    "$$\\lim_{n\\to \\infty} \\left(1 + \\frac{x}{n}\\right)^n = e^{x}$$\n",
    "\n",
    "or better: $$\\lim_{n\\to \\infty} \\left(1 + \\frac{x + g_n(x)}{n}\\right)^n = e^x ~~ \\text{ if } ~~ g_n(x) \\to 0 \\text{ as }n\\to \\infty \\text{ uniformly in }x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, $g_n(x) = \\epsilon(\\frac{t}{\\sqrt{n}}) t^2$ and $x=\\frac{1}{2} t^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some points to consider: **\n",
    "\n",
    " * If by mistake, we write $S_n = \\frac{Z_1 + Z_2 + ... + Z_n}{n}$ instead of $S_n = \\frac{Z_1 + Z_2 + ... + Z_n}{\\sqrt{n}}$, then $S_n$ is the empirical mean of $Z_i$'s and according to the **law of large numbers** it converges to zero.\n",
    " \n",
    " * If $S_n = \\frac{Z_1 + Z_2 + ... + Z_n}{n^\\frac{1}{4}}$, then $S_n \\to \\infty$ not normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applicaiton:\n",
    "\n",
    "Let $X_n$ be a $Binomial(n,p)$. \n",
    "\n",
    " * Recall that $X_n$ is the sum of $n$ Bernoulli random variables with parameter $p$. $X_n = r_1 + .. r_n$\n",
    "\n",
    "\n",
    "Let $S_n = \\displaystyle \\frac{X_n - np}{\\sqrt{n} ~\\sqrt{p(1-p)}}$\n",
    "\n",
    "By CLT, $S_n \\to \\mathcal{N}(0,1) \\text{ as } n\\to \\infty$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words:\n",
    "\n",
    "The fluctuations of the number of successes in $n$ independent trials with success probability $p$, are of order $\\sqrt{n}$ and within that scale. The size of each flucutation is approximately of normal (Gaussian) law with variance $p(1-p)$ \n",
    "\n",
    "In other words: $$Binomial(n,p) \\approx np + \\sqrt{n} \\mathcal{N}\\left(0, p(1-p)\\right) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0rc4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
