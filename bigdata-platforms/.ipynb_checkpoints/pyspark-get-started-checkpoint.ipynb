{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark for Data Analysis\n",
    "============\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Basic Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext \n",
    "sc = SparkContext( 'local', 'pyspark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[1] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_RDD = sc.parallelize(range(10), 3)\n",
    "\n",
    "int_RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_RDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2], [3, 4, 5], [6, 7, 8, 9]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_RDD.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data from a text file\n",
    "\n",
    "To read data from a local file, you need to specify the address by **file://**\n",
    "\n",
    "```\n",
    "   textFile(\"file:///home/vahid/examplefile.txt\")\n",
    "```\n",
    "\n",
    "But if the file is on HDFS, then we can specify the address by\n",
    "```\n",
    "    textFile(\"/user/wordcount/input/examplefile.txt\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[20] at textFile at NativeMethodAccessorImpl.java:-2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = sc.textFile('file:///home/vahid/Github/DataScience/bigdata-platforms/data/31987-0.txt')\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take the first k elements (lines)\n",
    "\n",
    "```\n",
    "text.take(k)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Project Gutenberg EBook of Territory in Bird Life, by H. Eliot Howard',\n",
       " '',\n",
       " 'This eBook is for the use of anyone anywhere at no cost and with']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Narrow Transformation\n",
    "\n",
    "\n",
    "  * **map:** applies a function to each element of RDD.\n",
    "\n",
    "  * **flatMap:** similar to map, except that here we can have 0 or more outputs for each element\n",
    "  \n",
    "  * **filter:** apply a boolean function to each element of RDD, resulting in filtering out based on that function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Love looks not with the eyes, but with the mind; and therefore is winged Cupid painted blind.']\n"
     ]
    }
   ],
   "source": [
    "example = sc.textFile('data/example.txt')\n",
    "\n",
    "# print the first line to make sure it's working\n",
    "print(example.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['love looks not with the eyes, but with the mind; and therefore is winged cupid painted blind.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lower(line):\n",
    "    return(line.lower())\n",
    "\n",
    "# apply lower() to each element:\n",
    "example.map(lower).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Love', 'looks', 'not', 'with', 'the']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split(line):\n",
    "    return(line.split())\n",
    "\n",
    "# apply split to each element, resulting in 0-more outputs --> flatMap\n",
    "example.flatMap(split).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Love', 1), ('looks', 1), ('not', 1), ('with', 1), ('the', 1)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_keyval(word):\n",
    "    return(word, 1)\n",
    "\n",
    "# Create key-value pairs for each split element --> map\n",
    "example.flatMap(split).map(create_keyval).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['therefore',\n",
       " 'winged',\n",
       " 'painted',\n",
       " 'blind.',\n",
       " 'dreams',\n",
       " 'little',\n",
       " 'rounded',\n",
       " 'sleep.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filterlen(word):\n",
    "    return(len(word)>5)\n",
    "\n",
    "# filter split elements based on their character lengths\n",
    "example.flatMap(split).filter(filterlen).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wide Transformation\n",
    " \n",
    "  * **groupByKey:**\n",
    "  * **reduceByKey:**\n",
    "  * **repartition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on, [1]\n",
      "stuff [1]\n",
      "few, [1]\n",
      "none. [1]\n",
      "a [1, 1]\n"
     ]
    }
   ],
   "source": [
    "pairs_RDD = example.flatMap(split).map(create_keyval)\n",
    "\n",
    "for key,vals in pairs_RDD.groupByKey().take(5):\n",
    "    print(key, list(vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('on,', 1),\n",
       " ('stuff', 1),\n",
       " ('few,', 1),\n",
       " ('none.', 1),\n",
       " ('a', 2),\n",
       " ('all,', 1),\n",
       " ('to', 1),\n",
       " ('with', 3),\n",
       " ('but', 1),\n",
       " ('Love', 2)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sumvals(a, b):\n",
    "    return (a + b)\n",
    "\n",
    "pairs_RDD.reduceByKey(sumvals).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: Installing Spark on Ubuntu\n",
    "\n",
    "\n",
    "Dowload and extract the spark package from \n",
    "\n",
    "\n",
    "```\n",
    "tar xvfz spark-1.5.2-bin-hadoop2.6.tgz\n",
    "sudo mv spark-1.5.2-bin-hadoop2.6 $HOME/apps/spark/\n",
    "cd $HOME/apps/spark/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to add the SPARK_HOME location to the PATH environment variable\n",
    "\n",
    "```\n",
    "export SPARK_HOME=$HOME/apps/spark \n",
    "export PATH=$SPARK_HOME/bin:$PATH \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can launch pyspark by ```pyspark```\n",
    "\n",
    "<img src='pyspark-launch.png' >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce the verbosity level\n",
    "\n",
    "By default, pyspark will generate lots of log messages when you run some command, and we can see how that can be a problem. To reduce the verbosity, copy the template file in the conf folder \n",
    "\n",
    "```cp $SPARK_HOME/conf/log4j.properties.template $SPARK_HOME/conf/log4j.properties```\n",
    "\n",
    "and edit it by replacing the INFO to WARN.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Using pyspark in iPython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In order to use pyspark in an iPython notebook, you need to configure it by adding a new file in the startup directory of ipython profile.\n",
    "\n",
    "```\n",
    "vim $HOME/.ipython/profile_default/startup/00-pyspark-setup.py\n",
    "```\n",
    "and add these contents in this file:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Configure the environment\n",
    "if 'SPARK_HOME' not in os.environ:\n",
    "    home_folder = os.environ['HOME']\n",
    "    os.environ['SPARK_HOME'] = os.path.join(home_folder, 'apps/spark')\n",
    "\n",
    "# Create a variable for our root path\n",
    "SPARK_HOME = os.environ['SPARK_HOME']\n",
    "\n",
    "# Add the PySpark/py4j to the Python Path\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, \"python\", \"build\"))\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, \"python\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, you should be able to run ipython and use pyspark. Try running the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vahid/apps/spark\n"
     ]
    }
   ],
   "source": [
    "print(SPARK_HOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[1] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext( 'local', 'pyspark')\n",
    "\n",
    "sc.parallelize(range(10), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you received an error for py4j, then you also need to run these two commands in bash shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```\n",
    "export PYTHONPATH=$SPARK_HOME/python/:$PYTHONPATH\n",
    "export PYTHONPATH=$SPARK_HOME/python/lib/py4j-0.8.2.1-src.zip:$PYTHONPATH\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
