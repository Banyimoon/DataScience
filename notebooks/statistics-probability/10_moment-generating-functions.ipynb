{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moment Generating Functions and Limit Theory\n",
    "========\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MGF: Moment Generating Function**\n",
    "\n",
    "\n",
    "#### Definition\n",
    "The MGF of a random variable $X$ is:\n",
    "\n",
    "$$M_X(t) = \\mathbf{E}[e^{tX}]$$\n",
    "\n",
    "for any $t\\in \\mathbb{R}$ where the expectation exists.\n",
    "\n",
    "Some people call $M_X(.)$ the **Laplace transform** of the distribution of $X$.\n",
    "\n",
    "#### Definition\n",
    "The characteristic function of $X$ is $C(t) = \\displaystyle \\mathbf{E}[e^{itX}]$ where $i=\\sqrt{-1}$ and $t\\in \\mathbb{R}$.\n",
    "\n",
    "Some call this $X_X(.)$ the **Fourier transform** of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Algenraically, $C_X(t)=M_X(it)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________\n",
    "\n",
    "\n",
    "##### Example:\n",
    "\n",
    "$X\\sim Exponential(\\lambda=1)$\n",
    "\n",
    "$$M_X(t) = \\mathbf{E}[e^{tx}] = \\int_0^\\infty e^{-x} e^{tx}dx = \\int_0^\\infty e^{-(1-t)x} dx$$\n",
    "\n",
    "In order for the above integral to exists: $t < 1$\n",
    "\n",
    "and as a result, $\\displaystyle M_X(t) = \\frac{1}{1-t} ~\\ \\forall t<1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** If $Y\\sim Exponential(\\lambda)$, then $Y = \\frac{1}{\\lambda} X$ therefore:\n",
    "\n",
    "$$M_Y(t) = \\mathbf{E}[e^{t\\frac{X}{\\lambda}}] = M_X(\\frac{t}{\\lambda}) = \\frac{1}{1 - \\displaystyle \\frac{t}{\\lambda}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Poisson:  $N \\sim Poisson(\\lambda)$\n",
    "\n",
    "$$M_N(t) = \\mathbf{E}[e^{tN}] = \\sum_{n=0}^\\infty e^{tn} ~ e^{-\\lambda} ~ \\frac{\\lambda^n}{n!} \\\\ e^{-\\lambda} \\sum_{n=0}^\\infty \\frac{(e^t\\lambda)^n}{n!} \\\\= e^{-\\lambda}\\sum_{n=0}^\\infty \\frac{x^n}{n!} \\text{ where }x=e^t\\lambda$$\n",
    "\n",
    " * Reminder:  $\\displaystyle \\sum_{n=0}^\\infty \\frac{x^n}{n!} = e^x$\n",
    " \n",
    "So finally, we get: $$M_N(t) = e^{-\\lambda}e^x = e^{\\displaystyle \\lambda(e^t - 1)} \\ \\ \\forall t\\in \\mathbb{R}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trick:**  for finding the value of $\\displaystyle \\sum_{n=0}^\\infty \\frac{x^n}{n!} = ?$\n",
    "\n",
    "$\\sum_{n=0}^\\infty \\frac{x^n}{n!} = \\sum_{n=0}^\\infty e^{-x}\\frac{x^n}{n!} e^{x}$ and we know that $e^{-x}\\frac{x^n}{n!}$ is the PMF of Posson random variable, so sum over all $n$ PMFs should always be $1$, therefore, $\\displaystyle \\sum_{n=0}^\\infty \\frac{x^n}{n!} = e^x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proposition\n",
    "\n",
    "If $M_X(.)$ is defined near $0$ for every $t$ in an open interval containing $0$, then for any integer $k\\ge 1$: $$\\mathbf{E}[X^k] = \\frac{d^k~M_X}{dt^k}(t=0)$$\n",
    "\n",
    "The $k$th derivative of the function $M_X$ evaulated at $t=0$ is $=M_X^{(k)}(0)$.\n",
    "\n",
    "The $\\mathbf{E}[X^k]$ is called the moment of order $k$ for the distribution of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Proof:\n",
    "\n",
    "$$\\frac{d^kM_X}{dt^k}(t) = \\frac{d^k}{dt^k} \\mathbf{E}[e^{tX}] \\\\ = \\mathbf{E}[\\frac{d^k}{dt^k} e^{tX}] \\\\ = \\mathbf{E}[X^k e^{tX}]$$\n",
    "\n",
    "Then, we evaluate that at $t=0$: $$\\frac{d^k M_X}{dt^k}(t=0) = \\mathbf{E}[X^k e^0]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example: $k$th moment of Exponential\n",
    "\n",
    "For $X\\sim Exponential(1)$  we know $M_X(t) = \\frac{1}{1-t}$\n",
    "\n",
    "$$M'_X(t) = \\frac{1}{(1-t)^2}$$\n",
    "\n",
    "$$M''_X(t) = \\frac{2}{(1-t)^3}$$\n",
    "\n",
    "$$M^{(k)}_X(t) = \\frac{k!}{(1-t)^{k+1}}$$\n",
    "\n",
    "Therefore, by that proposition: \n",
    "\n",
    "$$\\mathbf{E}[X^k] = M_X^{(k)}(0) = k!$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example: how about $Exponential(\\lambda)$\n",
    "\n",
    "$Y = \\frac{X}{\\lambda}$\n",
    "\n",
    "$$\\mathbf{E}[Y^k]= \\mathbf{E}[\\left(\\frac{1}{\\lambda}X\\right)^k] = \\frac{k!}{\\lambda^k}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example: $N \\sim Poisson(\\lambda)$\n",
    "\n",
    "$N\\sim Poisson(\\lambda)$\n",
    "\n",
    "$\\mathbf{E}[N]  = M'_N(0)$ and we know that $M_N(t) = e^{\\displaystyle \\lambda(e^t-1)}$\n",
    "\n",
    "$M'_N(t) = \\lambda e^t e^{\\displaystyle \\lambda(e^t-1)}$\n",
    "\n",
    "\n",
    "$$\\mathbf{E}[N] = \\lambda e^0 e^{\\displaystyle \\lambda(e^0-1)} = \\lambda$$\n",
    "\n",
    "$\\displaystyle M''_N(t) = \\lambda e^t \\lambda e^t e^{\\displaystyle \\lambda(e^t-1)} + \\lambda e^t e^{\\displaystyle \\lambda(e^t-1)}$\n",
    "\n",
    "$$\\mathbf{E}[N^2] = \\lambda^2 + \\lambda$$\n",
    "\n",
    "So variance of $N$ is:\n",
    "\n",
    "$$\\mathbf{Var}[N] = \\lambda^2 + \\lambda - \\lambda^2 = \\lambda$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________\n",
    "\n",
    "_____________\n",
    "\n",
    "### Proposition\n",
    "\n",
    "\n",
    " * $M_{aX+b}(t) =e^{tb}~M_X(ta)$\n",
    " \n",
    " * If $X\\&Y$ are independent, then $\\displaystyle M_{X+Y}(t) = M_X(t) M_Y(t)$\n",
    " \n",
    " Indeed, $$\\mathbf{E}[e^{t(X+Y)}] = \\mathbf{E}[e^{tX} e^{tY}] = \\mathbf{E}[e^{tX}] \\mathbf{E}[e^{tY}]$$\n",
    " \n",
    "\n",
    "This works for a sum of $n$ independent terms. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Theorem\n",
    "\n",
    "$M_X$ characterizes the law of $X$; In other words, if $X\\&Y$ are two random variables, then if $M_X(t) = M_Y(t)$ for all $t$ near $0$, then $X\\&Y$ have the same distributions.\n",
    "\n",
    "\n",
    "Even better, **If $Y=X_1+X_2$ and $M_Y = M_X~M_Y$ then $X_1$ and $X_2$ are independent.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example:\n",
    "\n",
    "Let $X_1\\sim Bernoulli(p)$, then $M_{X_1}(t) = 1-p + pe^t$.\n",
    "\n",
    "Consequently, let $X\\sim Binomial(n,p)$. We know that $X$ can be reprsented as $X=X_1+X_2+...+X_n$ where $X_i$'s are $i.i.d.$ Bernoulli nrandom variables with parameter $p$. Therefore, $M_X(t) = \\prod_{i=1}^n M_{X_i}(t) = \\left(1-p+pe^t\\right)^n$\n",
    "\n",
    "**Note:** Let's say we proved independently that $M_X(t)$ has the above formula. Then, the \"even better\" statement above proves that $X=\\sum_i X_i$ with $iid$ Bernoulli random variable $X_i$'s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##### Example: Normal\n",
    "\n",
    "Let $Z\\sim \\mathcal{N}(0,1)$. Then, we know that density of $Z$ is $f_Z(z) = \\displaystyle \\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2}$. Then, it's not too hard to show that $M_Z(t) = e^{t^2/2}$\n",
    "\n",
    "\n",
    "$$Z\\sim \\mathcal{N}(0,1) \\Longrightarrow M_Z(t) = \\displaystyle  e^{\\frac{t^2}{2}} \\ \\forall t$$\n",
    "\n",
    "\n",
    "By the scaling rule: If $X\\sim \\mathcal{N}(\\mu,\\sigma^2)$ then \n",
    "\n",
    "$$X\\sim \\mathcal{N}(\\mu,\\sigma^2) \\Longrightarrow M_X(t) = \\displaystyle  e^{\\mu t + \\frac{\\sigma^2 t^2}{2}} \\ \\forall t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________\n",
    "\n",
    "____________________\n",
    "\n",
    "\n",
    "## Convergence Types\n",
    "\n",
    "### Central Limit Theorem\n",
    "\n",
    "**Note:** Let $X_i$ be $iid\\ \\ \\sim \\mathcal{N}(0,1)$. Let $S_n= \\sum_{i=1}^n X_i$.\n",
    "\n",
    "Let $Y_n = \\frac{S_n}{\\sqrt n}$. (we divided by the standard deviation of $S_n$)\n",
    "\n",
    "We know that $S_n \\sim \\mathcal{N}(0,n)$ therefore, $Y_n \\sim \\mathcal{N}(0,1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The standradized partial sums of the $X_i$'s is normal, i.e. $\\sim \\mathcal{N}(0,1)$.\n",
    "\n",
    "More generally, if the $X_i$'s are not normal, the nice conclusion remains true, but only approximately.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theorem (Central Limit Theorem)\n",
    "\n",
    "**The most theorem in probability**\n",
    "\n",
    "Let $X_i, i=1,2,...n,...$ b $iid$ and assume $\\sigma^2 = \\mathbf{Var}[X_i]< \\infty$. Notation: $\\mu=\\mathbf{E}[X_i]$\n",
    "\n",
    "Then, let $S_n=X_1+X_2+...+X_n - n\\mu$\n",
    "\n",
    "Let $Z_n=\\displaystyle \\frac{S_n}{\\sigma \\sqrt{n}}$\n",
    "\n",
    "So, we say \"$Z_n$ os the standardized partial sum od the sequence of $X_i$\"\n",
    "\n",
    "**Conclusion:** the CDF of $Z_n$ converges to the CDF of $\\mathcal{N}(0,1)$. \n",
    "\n",
    "We say $Z_n$ converges to $\\mathcal{N}(0,1)$. We can write:\n",
    "\n",
    "$$\\lim_{n\\to \\infty} \\mathbf{P}[Z\\le a] = \\int_{-\\infty}^a \\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2} dz$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some example properties of central limit theorem**\n",
    "\n",
    "##### Example: \n",
    "\n",
    "Let $Y_n$ be $\\sim \\Gamma(n,\\theta)$. Let $S_n=Y_n-n\\theta$. However, we can represent $Y_n$ as \n",
    "\n",
    "$$Y_n = X_1 + X_2 +... + X_n \\text{ where }X_i\\text{'s are iid }Exponential(\\lambda=\\frac{1}{\\theta})$$\n",
    "\n",
    "Therefore, $S_n$ is just like in the central limit theorem. Som by CLT, the CDF of $\\frac{Y_n-n\\theta}{\\sigma \\sqrt{n}}$ ( where $\\sigma = \\sqrt{\\mathbf{Var}[X_i]} = \\sqrt{(\\frac{1}{1/\\theta})^2} = \\theta$) is approximately the standard normal CDF.\n",
    "\n",
    "$$\\lim_{n\\to \\infty } \\mathbf{P}[\\frac{Y_n - n\\theta}{\\sigma \\sqrt{n}}\\le a] = \\int_{-\\inty}^a \\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2}~dz = F_{\\mathcal{N}(0,1)}(a)$$\n",
    "\n",
    "ofcourse, in this case, $\\sigma=\\theta$ so you can replace in formula above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example:\n",
    "\n",
    "Let $Y_n\\sim Binomial(n,p)$\n",
    "\n",
    "We know that $Y_n$ can be represneted as um of Bernoulli random variables: $Y_n = \\sum_i X_i$ with $X_i\\ iid \\sim Bernouli(p)$. Also, $\\mathbf{E}[X_i] = p$ and $\\mathbf{Var}[X_i] = p(1-p)$ \n",
    "\n",
    "Letting $\\sigma = \\sqrt{p(1-p)}$, we get the CDF of $\\frac{Y_n - np}{\\sigma \\sqrt{n}}$ is $\\approx $ CDF of $\\mathcal{N}(0,1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example:\n",
    "\n",
    "A pretty good rule of thumb for simulating an $\\mathcal{N}(0,1)$ random variable.\n",
    "\n",
    "Let $X_i$ ne $iid\\ Uniform(0,1)$. So, we have $\\mathbf{E}[X_i]=\\frac{1}{2}$ and $\\mathbf{Var}[X_i] = \\frac{1}{12}$.\n",
    "\n",
    "Let $S_{12} = X_1+X_2+...X_{12}$. Then, $\\mathbf{E}[S_{12}] = 6$ and $\\mathbf{E}[S_{12}] = 1$\n",
    "\n",
    "Therefore, $\\frac{S_{12}-6}{\\sqrt{1}} = S_{12}-6$ is approximately $\\mathcal{N}(0,1)$\n",
    "\n",
    "A slightly more accurate version is $$\\frac{S_{48}-24}{\\sqrt{4}} \\approx \\mathcal{N}(0,1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Major Question:\n",
    "\n",
    "We want the CDF of $\\frac{S_n - n\\mu}{\\sigma\\sqrt{n}}$ to be within an error $\\epsilon$ of CDF of $\\mathcal{N}(0,1)$. How big does $n$ needs to be to achieve this?\n",
    "\n",
    "**Theorem: (Berry Esseen)** If $\\mathbf{E}[|X_i|^3] = c< \\infty$ then the size of the error $\\epsilon \\le C_{univ} \\frac{C}{\\sigma^3 \\sqrt{n}}$\n",
    "\n",
    "$C_{univ}$ is a universal constant $\\le \\frac{1}{2}$.\n",
    "\n",
    "To be safe, we should choose $n$ such that $C_{univ}\\frac{C}{\\sigma^3 \\sqrt{n}} \\le \\text{out prefferred error tolerance, let's call it }\\delta$. \n",
    "\n",
    "So we need $\\sqrt{N} \\ge \\frac{C.C_{univ}}{\\sigma^3 \\delta}$ and therefore, $n \\ge \\frac{C^2 C_{univ}^2}{\\sigma^6 \\delta^2}$\n",
    "\n",
    "So, if $C\\& \\sigma^2$ are all in order of $1$ for example, we see that to get a tolerance of $\\delta$ of about $\\frac{1}{1000}$ we will need $10^6$ terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0rc4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
