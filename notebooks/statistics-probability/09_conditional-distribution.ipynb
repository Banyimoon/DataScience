{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional Distribution\n",
    "============="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition\n",
    "\n",
    "Let $X$ and $Y$ be two **discrete** random variables, with probability mass functions: $P_X$ and $P_Y$. Then, the conditional probability mass function of $Y$ given $X$ is the following:\n",
    "\n",
    "$$P_{Y|X} (y|x) = \\mathbf{P}[Y = y | X = x] = \\frac{\\mathbf{P}[Y=y \\text{ and }X = x]}{\\mathbf{P}[X=x]} = \\frac{P_{X,Y}(x,y)}{P_X(x)}$$\n",
    "\n",
    "where $P_{X,Y}(x,y)$ is the joint-probability mass function of $X,Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition\n",
    "\n",
    "Assume $X\\&Y$ are **continuous** random variables with $f_X=\\text{marginal density of }X$ and $f_{X,Y} = \\text{joint density of }(X,Y)$, then, the conditional density of $Y$ given $X$ is \n",
    "\n",
    "$$f_{Y|X}(y|x) = \\frac{f_{X,Y}(x,y)}{f_X(x)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Intuition: why does it make sense?\n",
    "\n",
    "We re-interpret the left-hand side as follows. $\\displaystyle \\frac{\\mathbf{P}[Y\\in dy \\cap X\\in dx]/(dx~dy)}{\\mathbf{P}[X \\in dx]/dx} = \\frac{f_{X,Y}(x,y)~dx~dy/(dx~dy)}{f_X(x) dx / dx}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** It is easy to find the joint density if we know the conditional density and marginal density. \n",
    "\n",
    "Indeed: $f_{X,Y}(x,y) = f_X(x) f_{Y|X}(y|x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "_______________\n",
    "\n",
    "_______________\n",
    "\n",
    "For now, let's concentrate on the discrete case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example:\n",
    "\n",
    "$X_1 \\sim Poi(\\lambda_1)$ and $X_2 \\sim Poi(\\lambda_2)$ and $X_1,X_2$ are independent.\n",
    "\n",
    "Let $T=X_1+X_2$. We have $T \\sim Poi(\\lambda_1 + \\lambda_2)$\n",
    "\n",
    "Find the conditional distribution of $X_1$ given $T$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This question applies to any scenario where we observe the total number of events but we don't know exactly how many events fall into each of several categories.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the $\\displaystyle P_{X_1|T}(x_1 | t) = \\mathbf{P}[X_1 = x_1 | T = t_1]$\n",
    "\n",
    "$$= \\frac{P_{X_1|T}(x_1|t)}{P_T(t)}$$\n",
    "\n",
    "  * The denominator is the Poisson with parmaeter $\\lambda_1 + \\lambda_2$\n",
    "  \n",
    "\n",
    "$$=\\frac{\\mathbf{P}[X_1 = x_1 \\& X_1+X_2 = t]}{\\displaystyle e^{-(\\lambda_1 + \\lambda_2)} \\frac{(\\lambda_1 + \\lambda_2)^t}{t!}}$$\n",
    "\n",
    "A powerfull little thing is that we can replace $X_1 + X_2 = t$ with $X_2 = t - x_1$\n",
    "\n",
    "$$=\\frac{\\mathbf{P}[X_1 = x_1 \\text{ AND } X_2 = t - x_1]}{\\displaystyle e^{-(\\lambda_1 + \\lambda_2)} \\frac{(\\lambda_1 + \\lambda_2)^t}{t!}}$$\n",
    "\n",
    "And remember that $X_1$ and $X_2$ are independent, so we write:\n",
    "\n",
    "$$=\\frac{\\mathbf{P}[X_1 = x_1\\ \\times  \\mathbf{P}[X_2 = t - x_1]}{\\displaystyle e^{-(\\lambda_1 + \\lambda_2)} \\frac{(\\lambda_1 + \\lambda_2)^t}{t!}}$$\n",
    "\n",
    "And we know the probability mass functions of each of $X_1$ and $X_2$, which is Poisson:\n",
    "\n",
    "$$=\\frac{ \\displaystyle  e^{-\\lambda_1} \\frac{\\lambda_1^{x_1}}{x_1!} \\times e^{-\\lambda_2} \\frac{\\lambda_2^{t-x_1}}{(t-x_1)!}}{\\displaystyle e^{-(\\lambda_1 + \\lambda_2)} \\frac{(\\lambda_1 + \\lambda_2)^t}{t!}}$$\n",
    "\n",
    "$$=\\frac{t!}{x_1! (t-x_1)!} \\times \\frac{\\lambda_1^{x_1} \\lambda_2^{t-x_1}}{(\\lambda_1+\\lambda_2)^t}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last results looks pretty much similar to **binomial** distribution. Let's simplify it further\n",
    "\n",
    "$$\\left(\\begin{array}{c}t\\\\x_1\\end{array}\\right) \\times \\left(\\begin{array}{c}\\frac{\\lambda_1}{\\lambda_1 + \\lambda_2}\\end{array}\\right)^x_1 \\times \\left(\\begin{array}{c}\\frac{\\lambda_2}{\\lambda_1 + \\lambda_2}\\end{array}\\right)^{t-x_1}$$\n",
    "\n",
    "We can see that $p=\\displaystyle \\left(\\begin{array}{c}\\frac{\\lambda_1}{\\lambda_1 + \\lambda_2}\\end{array}\\right)$ and $1-p=\\left(\\begin{array}{c}\\frac{\\lambda_2}{\\lambda_1 + \\lambda_2}\\end{array}\\right)$\n",
    "\n",
    "We proved that $X_1 \\text{ given } T=t ~~ \\sim Binomial(n=t, p=\\frac{\\lambda_1}{\\lambda_1 + \\lambda_2})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example:\n",
    "\n",
    "Similar to the previous one (opposite in some sence): **mixture distributions**\n",
    "\n",
    "Assume $T\\sim Poi(\\lambda)$. And that $\\text{given }T=t, \\ \\ X\\sim Binom(t, \\theta)$\n",
    "\n",
    "Question: What is the (non-conditional) distribution of $X$ by itself?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: $X\\sim Poi(p\\lambda)$\n",
    "\n",
    "The reason that we say $X$ has mixture distribution is because one of the parameters in its (conditional) distribution is a random variable. Symbollically, we can write: $$X\\sim Binom(T, p)\\text{ where }T\\sim Poi(\\lambda)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic Other Way of Using Conditional Distribution: Discrete Markov Chain\n",
    "\n",
    "\n",
    " * Construction: A Markov chain is a stochastic process $\\{X(t): ~ t\\in N\\}$\n",
    "We want to define the joint distribution of all the $X(t)$'s for all $t$'s simultaneously. \n",
    "\n",
    " * Prescription: Given conditional distribution of $X(t+1)$ given $X(t)$, we also insist that this conditional distribution is the same as $X(t+1)$ given all $X(s)$'s for $s\\le t$.\n",
    " \n",
    " * Notation: the $X(t)$'s all take values in the ***\"state space\"*** $I=\\{x_1,x_2,...x_m\\}$\n",
    " \n",
    " We only need to prescribe these values:\n",
    " \n",
    " $$\\mathbf{P}[X(t+1) = x_j | X(t)=x_i] = P_{ij}$$\n",
    " \n",
    "These $P_{ij}$ are called ***transition probabilities***. These are arranged in a matrix $\\mathbf{P}$ which is the ***transition matrix***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example: Random Walk\n",
    "\n",
    "Let $Y_0,Y_1,Y_2,...,Y_n,...$ be $ii$, with $\\displaystyle Y_i = \\left\\{\\begin{array}{lrr}1 & \\text{with probability} & p\\\\-1 & \\text{with probability} & 1-p\\end{array}\\right.$\n",
    "\n",
    "Define $X(0)=0$ and $\\forall t\\ge 0, X(t+1) = X(t) + Y_{t}$. Therefore, we notice that $X(t) = Y_0+Y_1 + Y_2 + ... +Y_{t-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this definition, we see that $X(t)$ is a Markov chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0rc4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
